#!/usr/bin/env python3
"""
è¯„ä¼° ground_truth ç›®å½•ä¸‹çš„ CodeBLEU ç»“æœ
ä»¿ç…§ full_process.py çš„è°ƒç”¨æ–¹å¼

å‚è€ƒä»£ç æ¥æº: dataset/generation_context_ground_truth/[é¡¹ç›®å]/*.st
ç”Ÿæˆä»£ç æ¥æº: ground_truth/[é¡¹ç›®ç›®å½•]/readful_result/*.st
"""

import sys
import json
from pathlib import Path
from datetime import datetime

# æ·»åŠ  evaluate ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "evaluate"))

from codebleu_evaluator import evaluate_and_save


def evaluate_ground_truth_project(project_dir: Path, lang: str = "python", use_project_code: bool = False) -> bool:
    """
    è¯„ä¼°å•ä¸ª ground_truth é¡¹ç›®çš„ CodeBLEU
    
    å‚æ•°:
        project_dir: ground_truth å­ç›®å½•è·¯å¾„
        lang: ç¼–ç¨‹è¯­è¨€ï¼ˆé»˜è®¤ pythonï¼‰
        use_project_code: æ˜¯å¦ä½¿ç”¨ project_code ä½œä¸ºå‚è€ƒï¼ˆé»˜è®¤ Falseï¼‰
            - False: ä½¿ç”¨ generation_context_ground_truthï¼ˆé»˜è®¤ï¼‰
            - True: ä½¿ç”¨ project_code/[é¡¹ç›®å]/FUN/
    
    è¿”å›:
        æ˜¯å¦è¯„ä¼°æˆåŠŸ
    """
    print(f"\n{'='*80}")
    print(f"è¯„ä¼°é¡¹ç›®: {project_dir.name}")
    print(f"{'='*80}")
    
    # æ£€æŸ¥ readful_result ç›®å½•æ˜¯å¦å­˜åœ¨
    readful_result_dir = project_dir / "readful_result"
    
    if not readful_result_dir.exists():
        print(f"  âš ï¸  è·³è¿‡: æœªæ‰¾åˆ° readful_result ç›®å½•")
        return False
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ ST æ–‡ä»¶
    st_files = list(readful_result_dir.glob("*.st"))
    if not st_files:
        print(f"  âš ï¸  è·³è¿‡: readful_result ç›®å½•ä¸­æ²¡æœ‰ ST æ–‡ä»¶")
        return False
    
    print(f"  âœ“ æ‰¾åˆ° {len(st_files)} ä¸ª ST æ–‡ä»¶")
    
    # è°ƒç”¨ evaluate_and_save è¿›è¡Œè¯„ä¼°
    try:
        eval_success = evaluate_and_save(
            project_dir,
            output_filename="codebleu_evaluation.json",
            lang=lang,
            use_project_code=use_project_code
        )
        
        if eval_success:
            # è¯»å–å¹¶æ˜¾ç¤ºè¯„ä¼°ç»“æœæ‘˜è¦
            eval_file = project_dir / "codebleu_evaluation.json"
            if eval_file.exists():
                with open(eval_file, 'r', encoding='utf-8') as f:
                    result = json.load(f)
                
                print(f"\n  ğŸ“Š è¯„ä¼°æ‘˜è¦:")
                print(f"     é¡¹ç›®åç§°: {result['project_name']}")
                print(f"     åŸå§‹é¡¹ç›®: {result['original_project_name']}")
                print(f"     è¯„ä¼°æ–‡ä»¶æ•°: {result['successful_evaluations']}/{result['total_files']}")
                print(f"     å¹³å‡ CodeBLEU: {result['average_scores']['codebleu']:.4f}")
                
                print(f"\n  ğŸ’¾ ç»“æœå·²ä¿å­˜: {eval_file}")
            
            return True
        else:
            print(f"  âš ï¸  è¯„ä¼°æœªå®Œæˆ")
            return False
            
    except Exception as e:
        print(f"  âŒ è¯„ä¼°å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»å‡½æ•°ï¼šè¯„ä¼° ground_truth ç›®å½•ä¸‹çš„æ‰€æœ‰é¡¹ç›®"""
    import argparse
    
    parser = argparse.ArgumentParser(description="è¯„ä¼°ç›®å½•ä¸‹çš„ CodeBLEU ç»“æœ")
    parser.add_argument(
        "--dir",
        type=str,
        default="ground_truth",
        help="æŒ‡å®šè¦è¯„ä¼°çš„ç›®å½•è·¯å¾„ï¼ˆä¾‹å¦‚: ground_truth, after_gen_gt, ground_truth/repoeval_readwriteFileï¼‰"
    )
    parser.add_argument(
        "--lang",
        type=str,
        default="python",
        help="ç¼–ç¨‹è¯­è¨€ï¼ˆé»˜è®¤: pythonï¼Œç”¨äº ST ä»£ç çš„è¿‘ä¼¼è¯„ä¼°ï¼‰"
    )
    parser.add_argument(
        "--use_project_code_gt",
        action="store_true",
        help="ä½¿ç”¨ project_code/[é¡¹ç›®å]/FUN/ ä½œä¸ºå‚è€ƒä»£ç ï¼ˆé»˜è®¤ä½¿ç”¨ generation_context_ground_truthï¼‰"
    )
    
    args = parser.parse_args()
    
    # è·å–é¡¹ç›®æ ¹ç›®å½•
    repo_root = Path(__file__).parent
    target_root = repo_root / args.dir
    
    if not target_root.exists():
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {target_root}")
        return 1
    
    print("="*80)
    print("CodeBLEU è¯„ä¼°")
    print("="*80)
    print(f"ç›®æ ‡ç›®å½•: {target_root}")
    
    # æ˜¾ç¤ºå‚è€ƒä»£ç æ¥æº
    if args.use_project_code_gt:
        print(f"å‚è€ƒä»£ç æ¥æº: dataset/project_code/[é¡¹ç›®å]/FUN/")
    else:
        print(f"å‚è€ƒä»£ç æ¥æº: dataset/generation_context_ground_truth/[é¡¹ç›®å]/")
    
    # ç¡®å®šè¦è¯„ä¼°çš„é¡¹ç›®
    if target_root.is_file() or (target_root / "readful_result").exists():
        # å¦‚æœç›®æ ‡æ˜¯å•ä¸ªé¡¹ç›®ç›®å½•ï¼ˆåŒ…å« readful_resultï¼‰
        project_dirs = [target_root]
        print(f"\nè¯„ä¼°å•ä¸ªé¡¹ç›®: {target_root.name}")
    else:
        # å¦åˆ™ï¼Œè¯„ä¼°ç›®å½•ä¸‹æ‰€æœ‰åŒ…å« readful_result çš„å­ç›®å½•
        project_dirs = [
            d for d in target_root.iterdir()
            if d.is_dir() and (d / "readful_result").exists()
        ]
        
        if not project_dirs:
            print(f"\nâŒ ground_truth ç›®å½•ä¸‹æ²¡æœ‰å­ç›®å½•")
            return 1
        
        print(f"\næ‰¾åˆ° {len(project_dirs)} ä¸ªé¡¹ç›®ï¼ˆä½¿ç”¨ --all è¯„ä¼°æ‰€æœ‰é¡¹ç›®ï¼‰")
        print(f"é»˜è®¤è¯„ä¼°æ‰€æœ‰é¡¹ç›®...")
    
    # è®°å½•è¯„ä¼°ç»“æœ
    results = {
        "success": [],
        "failed": [],
        "skipped": []
    }
    
    # è¯„ä¼°ç»Ÿè®¡
    evaluation_stats = {}
    
    print(f"\nå¼€å§‹è¯„ä¼°...")
    print("="*80)
    
    # å¯¹æ¯ä¸ªé¡¹ç›®è¿›è¡Œè¯„ä¼°
    for idx, project_dir in enumerate(sorted(project_dirs), 1):
        project_name = project_dir.name
        
        print(f"\n[{idx}/{len(project_dirs)}] å¤„ç†é¡¹ç›®: {project_name}")
        
        success = evaluate_ground_truth_project(project_dir, lang=args.lang, use_project_code=args.use_project_code_gt)
        
        if success:
            results["success"].append(project_name)
            
            # è¯»å–è¯„ä¼°ç»“æœç»Ÿè®¡
            eval_file = project_dir / "codebleu_evaluation.json"
            if eval_file.exists():
                with open(eval_file, 'r', encoding='utf-8') as f:
                    eval_data = json.load(f)
                
                avg_scores = eval_data.get("average_scores", {})
                evaluation_stats[project_name] = {
                    "total_files": eval_data.get("total_files", 0),
                    "successful_evaluations": eval_data.get("successful_evaluations", 0),
                    "average_scores": {
                        "codebleu": avg_scores.get("codebleu", 0.0),
                        "ngram_match_score": avg_scores.get("ngram_match_score", 0.0),
                        "weighted_ngram_match_score": avg_scores.get("weighted_ngram_match_score", 0.0),
                        "syntax_match_score": avg_scores.get("syntax_match_score", 0.0),
                        "dataflow_match_score": avg_scores.get("dataflow_match_score", 0.0)
                    }
                }
        else:
            results["failed"].append(project_name)
    
    # è¾“å‡ºæ€»ç»“
    print("\n" + "="*80)
    print("è¯„ä¼°æ€»ç»“")
    print("="*80)
    print(f"\næ€»é¡¹ç›®æ•°: {len(project_dirs)}")
    print(f"æˆåŠŸ: {len(results['success'])}")
    print(f"å¤±è´¥: {len(results['failed'])}")
    
    if results["success"]:
        print(f"\nâœ… æˆåŠŸè¯„ä¼°çš„é¡¹ç›® ({len(results['success'])}):")
        for project in results["success"]:
            stats = evaluation_stats.get(project, {})
            total = stats.get("total_files", 0)
            success_count = stats.get("successful_evaluations", 0)
            avg_scores = stats.get("average_scores", {})
            print(f"  - {project}: {success_count}/{total} æ–‡ä»¶")
            print(f"      CodeBLEU: {avg_scores.get('codebleu', 0.0):.4f}")
            print(f"      N-gram: {avg_scores.get('ngram_match_score', 0.0):.4f}, "
                  f"Weighted N-gram: {avg_scores.get('weighted_ngram_match_score', 0.0):.4f}")
            print(f"      Syntax: {avg_scores.get('syntax_match_score', 0.0):.4f}, "
                  f"Dataflow: {avg_scores.get('dataflow_match_score', 0.0):.4f}")
    
    if results["failed"]:
        print(f"\nâŒ å¤±è´¥çš„é¡¹ç›® ({len(results['failed'])}):")
        for project in results["failed"]:
            print(f"  - {project}")
    
    # è®¡ç®—æ€»ä½“ç»Ÿè®¡
    overall_stats = {}
    if evaluation_stats:
        total_files = sum(s["successful_evaluations"] for s in evaluation_stats.values())
        
        # è®¡ç®—å„é¡¹æŒ‡æ ‡çš„åŠ æƒå¹³å‡
        metrics = ["codebleu", "ngram_match_score", "weighted_ngram_match_score", 
                  "syntax_match_score", "dataflow_match_score"]
        
        overall_averages = {}
        for metric in metrics:
            total_weighted_score = sum(
                s["average_scores"][metric] * s["successful_evaluations"] 
                for s in evaluation_stats.values()
            )
            overall_averages[metric] = total_weighted_score / total_files if total_files > 0 else 0.0
        
        overall_stats = {
            "total_files": total_files,
            "total_projects": len(evaluation_stats),
            "average_scores": overall_averages
        }
        
        print(f"\n{'='*80}")
        print("æ€»ä½“ç»Ÿè®¡")
        print(f"{'='*80}")
        print(f"æ€»é¡¹ç›®æ•°: {len(evaluation_stats)}")
        print(f"æ€»æ–‡ä»¶æ•°: {total_files}")
        print(f"\nå¹³å‡åˆ†æ•°:")
        print(f"  CodeBLEU:               {overall_averages['codebleu']:.4f}")
        print(f"  N-gram Match:           {overall_averages['ngram_match_score']:.4f}")
        print(f"  Weighted N-gram Match:  {overall_averages['weighted_ngram_match_score']:.4f}")
        print(f"  Syntax Match:           {overall_averages['syntax_match_score']:.4f}")
        print(f"  Dataflow Match:         {overall_averages['dataflow_match_score']:.4f}")
    
    # ä¿å­˜æ€»ç»“ç»“æœ
    summary_data = {
        "timestamp": datetime.now().isoformat(),
        "language": args.lang,
        "total_projects": len(project_dirs),
        "success_count": len(results["success"]),
        "failed_count": len(results["failed"]),
        "overall_statistics": overall_stats,
        "results": results,
        "project_statistics": evaluation_stats
    }
    
    # 1. ä¿å­˜å›ºå®šåç§°çš„æ–‡ä»¶ï¼ˆä¼šè¦†ç›–æ—§æ–‡ä»¶ï¼Œæ–¹ä¾¿æŸ¥æ‰¾æœ€æ–°ç»“æœï¼‰
    fixed_summary_file = target_root / "evaluation_results.json"
    with open(fixed_summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary_data, f, indent=2, ensure_ascii=False)
    print(f"\nğŸ’¾ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {fixed_summary_file}")
    
    # 2. åŒæ—¶ä¿å­˜å¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶ï¼ˆç”¨äºå†å²è®°å½•ï¼‰
    timestamped_summary_file = target_root / f"evaluation_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(timestamped_summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary_data, f, indent=2, ensure_ascii=False)
    print(f"ğŸ’¾ å†å²è®°å½•å·²ä¿å­˜åˆ°: {timestamped_summary_file}")
    
    print(f"\n{'='*80}")
    if results["failed"]:
        print("è¯„ä¼°å®Œæˆï¼ˆéƒ¨åˆ†å¤±è´¥ï¼‰")
        return 1
    else:
        print("âœ… è¯„ä¼°å®Œæˆï¼ˆå…¨éƒ¨æˆåŠŸï¼‰")
        return 0


if __name__ == "__main__":
    sys.exit(main())

