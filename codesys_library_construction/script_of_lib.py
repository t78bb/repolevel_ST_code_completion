#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time, json, os, re, sys
import argparse
from datetime import datetime
import random

extract_libraries = ["3S Storage", "3SLicense", "AC_DataLog", "AC_Alarming", "AC_ModuleBase", "AlarmManager", "Asynchronous Job Manager", "BACnet", "BACnet2", "Base Interfaces", "Building Automation", "CAA File", "CAA Real Time Clock Extern", "CAA Storage", "CANbus", "CANbusDevice", "CODESYS Safe Control", "CmpBACnet", "CmpCharDevice", "CmpCrypto", "CmpCrypto Interfaces", "CmpDNP3", "CmpDNP3 Interfaces" , "CmpDNP3 Implementation", "CmpCrypto Implementation", "CmpLog", "CmpWebServer", "CmpWebServer Implementation", "CmpWebServer_Itfs", "CommFB", "Common Behaviour Model", "Collections Interfaces", "Component Manager", "DHCP Client", "DNP3", "Data Server Interfaces", "Datasources", "Element Collections", "Empty", "ExtensionAPI", "FloatingPointUtils", "Generic String Base", "IoDriver Bus Control Interfaces", "IoDriver CIPService Interfaces", "IoDriver EIPAcyclicService Interfaces", "IoDriver Hilscher Interfaces", "IoDriver Parameter2 Interfaces", "IoDriver ProfiNet2 Interfaces", "IoDriver Profibus2 Interfaces", "IoDriver2 Interfaces", "IoDrvCIFXEthernetIP", "IoDrvCIFXProfibus", "IoDrvEtherCAT", "IoDrvEtherNetIP", "IoDrvEthernet", "IoDrvEthernet Interfaces", "IoDrvJ1939", "IoDrvKnxStack Interfaces", "IoDrvModbus", "IoDrvModbusBase", "IoDrvModbusSerial", "IoDrvModbusSerialServer", "IoDrvModbusSerialSlave", "IoDrvModbusTCP", "IoDrvModbusTCPServer", "IoDrvModbusTCPSlave", "IoDrvProfinet", "IoDrvProfinetBase", "IoDrvProfinetDevice", "IoStandard", "J1939 Safety", "J1939 Safety Interfaces", "J1939 Safety Standard", "MQTT Client SL", "Mail Service SL", "Matrix", "Memory Block Manager", "MemoryBarrier", "MemoryUtils", "ModbusFB", "ModbusFB non standard extensions", "ModbusTCP Server", "ModbusTCP Slave", "Net Base Services", "NotImplementedByDevice", "PLCopen Safety FBs", "Plc Services", "Profinet", "ProfinetCommon", "ProfinetDevice", "ProfinetDeviceConfig", "Recipe Management", "Redundancy Interfaces", "Redundancy Implementation", "RedundancyDataTransfer", "Remote Procedure Calls", "Rts Service Handler", "SDO Server", "SM3_Basic", "SM3_Basic_Visu", "SM3_CNC", "SM3_CNC_Visu", "SM3_CamBuilder"
"SM3_CommonPublic",
"SM3_Drive_CAN_Bonfiglioli_iBMD",
"SM3_Drive_CAN_CMZ_BD",
"SM3_Drive_CAN_CMZ_LBD",
"SM3_Drive_CAN_CMZ_SBD",
"SM3_Drive_CAN_CMZ_SD",
"SM3_Drive_CAN_Festo_CMMP",
"SM3_Drive_CAN_Festo_EMCA",
"SM3_Drive_CAN_INFRANOR",
"SM3_Drive_CAN_INFRANOR_CD1K",
"SM3_Drive_CAN_JAT",
"SM3_Drive_CAN_KEB",
"SM3_Drive_CAN_KEB_ITMotorB",
"SM3_Drive_CAN_KEB_SD",
"SM3_Drive_CAN_METRONIX",
"SM3_Drive_CAN_Maxon_EPOS4",
"SM3_Drive_CAN_Nanotec_PD4_C59",
"SM3_Drive_CAN_Schneider_Lexium05",
"SM3_Drive_CAN_Schneider_Lexium23",
"SM3_Drive_CAN_Schneider_Lexium28",
"SM3_Drive_CAN_Schneider_Lexium32",
"SM3_Drive_ETC",
"SM3_Drive_ETC_BRC_CtrlXDrive_CoE",
"SM3_Drive_ETC_BRC_CtrlXDrive_SoE",
"SM3_Dynamics",
"SM3_Error",
"SM3_Robotics",
"SM3_Robotics_Visu",
"SM3_Transformation",
"SML_Basic", "SMS Service SL", "SNCM Manager", "SNMP Service SL", "SNTP Service SL", "Standard", "Standard64", "Standard Monitoring Data Server Driver", "String Builder", "String Builder Base", "String Functions", "String Segments", "String Util Intern", "StringUtils", "SysDir", "SysFile", "SysEthernet", "SysFileAsync", "SysPipe Interfaces", "SysPipeWindows", "SysSem", "SysSem23", "SysSocket", "SysTime", "TCP", "Test Manager IEC Unit Test", "TextListUtils", "UDP", "UTF-16 Encoding Support", "Unicode Data", "Util", "Visu Interfaces", "Visu Utils", "VisuElemBase", "VisuGlobalClientManager", "VisuRedundancy", "VisuShared", "VisuUserMgmt", "Web Client SL", "iParServer"]


main_extract_libraries = ["BACnet","CommFB","DNP3","Empty","IoDrvEtherCAT", "IoStandard", "MQTT Client SL", "MemoryBarrier", "MemoryUtils", "ModbusFB", "PLCopen Safety FBs", "SM3_Basic", "SM3_Basic_Visu", "SM3_CNC", "SM3_CNC_Visu", "SM3_CommonPublic", "SM3_Drive_ETC", "SM3_Dynamics", "SM3_Error", "SM3_Robotics", "SM3_Robotics_Visu", "SML_Basic", "Standard", "Standard64", "String Builder", "String Builder Base", "String Functions", "String Segments", "StringUtils", "SysDir", "SysFile", "SysSem", "SysSocket", "SysTime", "Util", "Visu Utils"]



BASE = "https://content.helpme-codesys.com/en/libs/index.html"
ROOT_OUTPUT = "SCRIPT_LIBRARY"

HEADERS = {"User-Agent": "codesys-crawler/1.0 (+https://github.com)"}
session = requests.Session()
session.headers.update(HEADERS)

# åŒ¹é…åŒ…å«æ‹¬å·çš„é“¾æ¥æ–‡æœ¬ï¼Œå¦‚ "GetLibVersion (Function)", "AsyncProperty (FunctionBlock)" ç­‰
# ç®€åŒ–ï¼šç›´æ¥æ£€æŸ¥æ˜¯å¦åŒ…å«æ‹¬å·å¯¹
def is_target_link(txt):
    """æ£€æŸ¥é“¾æ¥æ–‡æœ¬æ˜¯å¦åŒ…å«æ‹¬å·ï¼ˆè¡¨ç¤ºæ˜¯ä¸€ä¸ªå¯æå–çš„ç›®æ ‡ï¼‰
    ä¾‹å¦‚: "GetLibVersion (Function)", "AsyncProperty (FunctionBlock)", "ERROR (Enum)" ç­‰
    """
    if not txt:
        return False
    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ‹¬å·å¯¹ï¼Œä¸”æ‹¬å·å†…æœ‰å†…å®¹
    if '(' in txt and ')' in txt:
        # ç¡®ä¿æ‹¬å·æ˜¯æˆå¯¹çš„ï¼Œä¸”æ‹¬å·å†…æœ‰å†…å®¹
        return bool(re.search(r'\([^)]+\)', txt))
    return False
MAX_DEPTH = 8
# å»¶è¿Ÿè®¾ç½®ï¼šåŸºç¡€å»¶è¿Ÿ + éšæœºå»¶è¿Ÿï¼Œé¿å…è¢«æ£€æµ‹ä¸ºçˆ¬è™«
SLEEP_MIN = 1.5  # æœ€å°å»¶è¿Ÿï¼ˆç§’ï¼‰
SLEEP_MAX = 3.0  # æœ€å¤§å»¶è¿Ÿï¼ˆç§’ï¼‰
MAX_RETRIES = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°
RETRY_DELAY = 5  # é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰


def safe_filename(s: str):
    return re.sub(r'[\\\\/:*?"<>|]', "_", s).strip()


def fetch(url, retry_count=0):
    """è·å–ç½‘é¡µå†…å®¹ï¼Œå¸¦é‡è¯•æœºåˆ¶å’Œéšæœºå»¶è¿Ÿ"""
    try:
        res = session.get(url, timeout=20)
        
        # æ£€æŸ¥HTTPçŠ¶æ€ç 
        if res.status_code == 429:  # Too Many Requests
            wait_time = RETRY_DELAY * (retry_count + 1)
            print(f"[WARN] è¯·æ±‚è¿‡äºé¢‘ç¹ (429)ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...", file=sys.stderr)
            time.sleep(wait_time)
            if retry_count < MAX_RETRIES:
                return fetch(url, retry_count + 1)
            else:
                print(f"[ERROR] é‡è¯•æ¬¡æ•°å·²è¾¾ä¸Šé™: {url}", file=sys.stderr)
                return None
        
        res.raise_for_status()
        
        # éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º
        sleep_time = random.uniform(SLEEP_MIN, SLEEP_MAX)
        time.sleep(sleep_time)
        
        return res.text
    
    except requests.exceptions.Timeout:
        if retry_count < MAX_RETRIES:
            print(f"[WARN] è¯·æ±‚è¶…æ—¶ï¼Œ{RETRY_DELAY}ç§’åé‡è¯• ({retry_count + 1}/{MAX_RETRIES}): {url}", file=sys.stderr)
            time.sleep(RETRY_DELAY)
            return fetch(url, retry_count + 1)
        else:
            print(f"[ERROR] è¯·æ±‚è¶…æ—¶ï¼Œé‡è¯•æ¬¡æ•°å·²è¾¾ä¸Šé™: {url}", file=sys.stderr)
            return None
    
    except requests.exceptions.HTTPError as e:
        if e.response.status_code == 429 and retry_count < MAX_RETRIES:
            wait_time = RETRY_DELAY * (retry_count + 1)
            print(f"[WARN] HTTPé”™è¯¯ {e.response.status_code}ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...", file=sys.stderr)
            time.sleep(wait_time)
            return fetch(url, retry_count + 1)
        else:
            print(f"[WARN] HTTPé”™è¯¯: {url} ({e})", file=sys.stderr)
            return None
    
    except Exception as e:
        if retry_count < MAX_RETRIES:
            print(f"[WARN] è¯·æ±‚å¤±è´¥ï¼Œ{RETRY_DELAY}ç§’åé‡è¯• ({retry_count + 1}/{MAX_RETRIES}): {url} ({e})", file=sys.stderr)
            time.sleep(RETRY_DELAY)
            return fetch(url, retry_count + 1)
        else:
            print(f"[ERROR] è¯·æ±‚å¤±è´¥ï¼Œé‡è¯•æ¬¡æ•°å·²è¾¾ä¸Šé™: {url} ({e})", file=sys.stderr)
            return None


def soupify(html):
    return BeautifulSoup(html, "html.parser")


def normalize(href, base):
    return urljoin(base, href) if href else None


def extract_tables(soup):
    tables_data = []
    for table in soup.find_all("table"):
        headers = []
        first_row = table.find("tr")
        if first_row:
            headers = [c.get_text(strip=True) for c in first_row.find_all(["th", "td"])]
        rows = []
        for tr in table.find_all("tr")[1:]:
            cells = [c.get_text(strip=True) for c in tr.find_all(["td", "th"])]
            if headers and len(cells) == len(headers):
                rows.append({h: v for h, v in zip(headers, cells)})
            else:
                rows.append(cells)
        tables_data.append({"headers": headers, "rows": rows})
    return tables_data


def extract_main_text(soup):
    for sel in ["main", "#content", ".content", ".article", ".page"]:
        found = soup.select_one(sel)
        if found and found.get_text(strip=True):
            return found.get_text("\n", strip=True)
    body = soup.body
    return body.get_text("\n", strip=True) if body else ""


def find_current_link(soup, base):
    for a in soup.find_all("a", href=True):
        txt = a.get_text(strip=True).lower()
        href = a["href"]
        if txt == "current" or "/current/" in href or "#current" in href:
            return normalize(href, base)
    return None


def find_library_links(index_soup):
    libs = []
    for a in index_soup.find_all("a", href=True):
        full = normalize(a["href"], BASE)
        if full and "/en/libs/" in full:
            libs.append((a.get_text(strip=True), full))
    seen, final = set(), []
    for title, url in libs:
        if url not in seen:
            seen.add(url)
            final.append((title, url))
    return final


def traverse_recursive(start_url):
    visited = set()
    targets = []
    seen_targets = set()  # ç”¨äºå»é‡ï¼Œé¿å…é‡å¤æ·»åŠ ç›¸åŒçš„ç›®æ ‡

    def _walk(url, depth):
        if depth > MAX_DEPTH or url in visited:
            return
        visited.add(url)
        # ğŸš¨ æ–°å¢ï¼šæ‰“å°æ¯ä¸€ä¸ªè®¿é—®åˆ°çš„é“¾æ¥
        print(url)

        html = fetch(url)
        if not html:
            return
        soup = soupify(html)
        for a in soup.find_all("a", href=True):
            txt = a.get_text(strip=True)
            full = normalize(a["href"], url)
            if not full:
                continue
            # æ£€æŸ¥é“¾æ¥æ–‡æœ¬æ˜¯å¦åŒ…å«æ‹¬å·ï¼ˆå¦‚ "(Function)", "(FunctionBlock)", "(Enum)" ç­‰ï¼‰
            if is_target_link(txt):
                # ä½¿ç”¨ URL ä½œä¸ºå”¯ä¸€æ ‡è¯†è¿›è¡Œå»é‡
                if full not in seen_targets:
                    seen_targets.add(full)
                    print(f"  [MATCHED] {txt} -> {full}")
                    targets.append({"title": txt, "url": full})
            else:
                p = urlparse(full)
                if p.netloc == urlparse(BASE).netloc and "/en/libs/" in p.path:
                    _walk(full, depth + 1)

    _walk(start_url, 0)
    return targets


def extract_page(item, library_name):
    html = fetch(item["url"])
    if not html:
        return None
    soup = soupify(html)
    tables = extract_tables(soup)
    text = extract_main_text(soup)
    return {
        "library": library_name,
        "title": item["title"],
        "url": item["url"],
        "page_title": soup.title.get_text(strip=True) if soup.title else item["title"],
        "text": text,
        "tables": tables
    }


def library_exists(library_name):
    """æ£€æŸ¥åº“ç›®å½•æ˜¯å¦å·²å­˜åœ¨ï¼ˆç”¨äºè·³è¿‡å·²æå–çš„åº“ï¼‰"""
    folder = os.path.join(ROOT_OUTPUT, safe_filename(library_name))
    return os.path.exists(folder) and os.path.isdir(folder)


def get_saved_files(library):
    """è·å–å·²ä¿å­˜çš„æ–‡ä»¶é›†åˆï¼ˆç”¨äºæ–­ç‚¹é‡ç»­ï¼‰"""
    folder = os.path.join(ROOT_OUTPUT, safe_filename(library))
    if not os.path.exists(folder):
        return set()
    saved = set()
    for fname in os.listdir(folder):
        if fname.endswith('.json'):
            # ç§»é™¤ .json åç¼€ï¼Œæ¢å¤åŸå§‹æ ‡é¢˜
            saved.add(fname[:-5])  # ä¿å­˜æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
    return saved


def save_json(obj, library):
    folder = os.path.join(ROOT_OUTPUT, safe_filename(library))
    os.makedirs(folder, exist_ok=True)
    fname = safe_filename(obj["title"]) + ".json"
    filepath = os.path.join(folder, fname)
    try:
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(obj, f, ensure_ascii=False, indent=2)
        print(f"    [ITEM SAVED] {obj['title']} -> {filepath}")
        return True
    except Exception as e:
        print(f"    [ERROR] Failed to save {obj['title']}: {e}", file=sys.stderr)
        return False


def main():
    start_time = time.time()
    
    parser = argparse.ArgumentParser(description="çˆ¬å– CODESYS åº“æ–‡æ¡£")
    parser.add_argument(
        "--library", "-l",
        type=str,
        default=None,
        help="æŒ‡å®šè¦æå–çš„åº“åï¼ˆæ”¯æŒéƒ¨åˆ†åŒ¹é…ï¼Œä¸æŒ‡å®šåˆ™æå–æ‰€æœ‰åº“ï¼‰"
    )
    parser.add_argument(
        "--use-list", "--list",
        action="store_true",
        help="ä½¿ç”¨ main_extract_libraries åˆ—è¡¨ä¸­å®šä¹‰çš„åº“è¿›è¡Œæå–"
    )
    parser.add_argument(
        "--resume",
        action="store_true",
        help="å¯ç”¨æ–­ç‚¹é‡ç»­åŠŸèƒ½ï¼Œè·³è¿‡å·²ä¿å­˜çš„æ–‡ä»¶"
    )
    args = parser.parse_args()

    os.makedirs(ROOT_OUTPUT, exist_ok=True)
    
    # å¦‚æœä½¿ç”¨åˆ—è¡¨æ¨¡å¼
    if args.use_list:
        print(f"ğŸ“‹ ä½¿ç”¨ main_extract_libraries åˆ—è¡¨ï¼Œå…± {len(main_extract_libraries)} ä¸ªåº“")
        html = fetch(BASE)
        if not html:
            print("âŒ Failed to fetch libs index page")
            return
        
        all_libs = find_library_links(soupify(html))
        # ä»æ‰€æœ‰åº“ä¸­åŒ¹é… main_extract_libraries ä¸­çš„åº“
        libs = []
        skipped_count = 0
        for lib_name in main_extract_libraries:
            if not lib_name.strip():  # è·³è¿‡ç©ºå­—ç¬¦ä¸²
                continue
            
            # æ£€æŸ¥åº“ç›®å½•æ˜¯å¦å·²å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™è·³è¿‡
            if library_exists(lib_name):
                print(f"â­ï¸  è·³è¿‡å·²æå–çš„åº“: '{lib_name}' (ç›®å½•å·²å­˜åœ¨)")
                skipped_count += 1
                continue
            
            matched = [(title, url) for title, url in all_libs if lib_name.lower() in title.lower() or title.lower() in lib_name.lower()]
            if matched:
                libs.extend(matched)
            else:
                print(f"âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°åº“ '{lib_name}'")
        
        if skipped_count > 0:
            print(f"ğŸ“Š å·²è·³è¿‡ {skipped_count} ä¸ªå·²æå–çš„åº“\n")
        
        if not libs:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•åŒ¹é…çš„åº“")
            return
        
        # å»é‡
        seen_urls = set()
        unique_libs = []
        for title, url in libs:
            if url not in seen_urls:
                seen_urls.add(url)
                unique_libs.append((title, url))
        libs = unique_libs
        
        print(f"ğŸ“Œ å°†å¤„ç† {len(libs)} ä¸ªåº“:\n")
        for title, _ in libs:
            print(f"  - {title}")
        print()
    
    # å¦‚æœæŒ‡å®šäº†å•ä¸ªåº“å
    elif args.library:
        html = fetch(BASE)
        if not html:
            print("âŒ Failed to fetch libs index page")
            return
        
        all_libs = find_library_links(soupify(html))
        print(f"ğŸ” Found {len(all_libs)} libraries\n")
        
        target_lib = args.library.lower()
        libs = [(title, url) for title, url in all_libs if target_lib in title.lower()]
        if not libs:
            print(f"âŒ æœªæ‰¾åˆ°åŒ¹é…çš„åº“: {args.library}")
            return
        print(f"ğŸ“Œ å°†å¤„ç† {len(libs)} ä¸ªåŒ¹é…çš„åº“:\n")
        for title, _ in libs:
            print(f"  - {title}")
        print()
    
    # å¦åˆ™æå–æ‰€æœ‰åº“
    else:
        html = fetch(BASE)
        if not html:
            print("âŒ Failed to fetch libs index page")
            return
        
        libs = find_library_links(soupify(html))
        print(f"ğŸ” Found {len(libs)} libraries\n")
        print("âš ï¸  æœªæŒ‡å®šåº“ï¼Œå°†æå–æ‰€æœ‰åº“\n")

    total_saved = 0
    total_skipped = 0
    
    for idx, (lib_title, lib_url) in enumerate(libs, 1):
        lib_start_time = time.time()
        print(f"\n[{idx}/{len(libs)}] [LIB] {lib_title}")
        
        # æ–­ç‚¹é‡ç»­ï¼šæ£€æŸ¥å·²ä¿å­˜çš„æ–‡ä»¶
        saved_files = set()
        if args.resume:
            saved_files = get_saved_files(lib_title)
            if saved_files:
                print(f"  ğŸ“‚ å‘ç° {len(saved_files)} ä¸ªå·²ä¿å­˜çš„æ–‡ä»¶ï¼Œå°†è·³è¿‡")
        
        html = fetch(lib_url)
        if not html:
            print(f"  âš ï¸  è·³è¿‡ï¼ˆæ— æ³•è·å–é¡µé¢ï¼‰")
            continue
        
        soup = soupify(html)
        start = find_current_link(soup, lib_url) or lib_url
        print(f"  ğŸ”— èµ·å§‹URL: {start}")
        
        targets = traverse_recursive(start)
        print(f"  â¤ æ‰¾åˆ° {len(targets)} ä¸ªç›®æ ‡é¡¹")
        
        lib_saved = 0
        lib_skipped = 0
        
        for item in targets:
            # æ–­ç‚¹é‡ç»­ï¼šæ£€æŸ¥æ˜¯å¦å·²ä¿å­˜
            item_filename = safe_filename(item["title"])
            if args.resume and item_filename in saved_files:
                lib_skipped += 1
                continue
            
            data = extract_page(item, lib_title)
            if data:
                if save_json(data, lib_title):
                    lib_saved += 1
                    total_saved += 1
                else:
                    lib_skipped += 1
                    total_skipped += 1
            else:
                lib_skipped += 1
                total_skipped += 1
        
        lib_elapsed = time.time() - lib_start_time
        print(f"  âœ… å®Œæˆ: ä¿å­˜ {lib_saved} ä¸ªï¼Œè·³è¿‡ {lib_skipped} ä¸ª (è€—æ—¶ {lib_elapsed:.2f}ç§’)")
        print("  --- Done ---\n")

    total_elapsed = time.time() - start_time
    hours = int(total_elapsed // 3600)
    minutes = int((total_elapsed % 3600) // 60)
    seconds = int(total_elapsed % 60)
    
    print("\n" + "="*60)
    print("ğŸ‰ çˆ¬å–å®Œæˆ!")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {ROOT_OUTPUT}")
    print(f"ğŸ“Š ç»Ÿè®¡: ä¿å­˜ {total_saved} ä¸ªæ–‡ä»¶ï¼Œè·³è¿‡ {total_skipped} ä¸ªæ–‡ä»¶")
    print(f"â±ï¸  æ€»è€—æ—¶: {hours}å°æ—¶ {minutes}åˆ†é’Ÿ {seconds}ç§’ ({total_elapsed:.2f}ç§’)")
    print("="*60)


if __name__ == "__main__":
    main()
