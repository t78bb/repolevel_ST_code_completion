"""
CodeBLEU è¯„ä¼°å™¨
ç”¨äºè¯„ä¼°ç”Ÿæˆä»£ç ä¸å‚è€ƒä»£ç çš„ç›¸ä¼¼åº¦
"""

import sys
import json
from pathlib import Path
from typing import Dict, List, Optional

# æ·»åŠ  codebleu-main åˆ°è·¯å¾„
REPO_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(REPO_ROOT / "codebleu-main"))

from codebleu import calc_codebleu


def get_original_project_name(repoeval_name: str) -> str:
    """
    ä» repoeval_ å‰ç¼€çš„åç§°ä¸­æå–åŸå§‹é¡¹ç›®å
    
    ä¾‹å¦‚: repoeval_three-axis_CNC_motion -> three-axis_CNC_motion
          repoeval_readwriteFile -> readwriteFile
    """
    if repoeval_name.startswith("repoeval_"):
        return repoeval_name[len("repoeval_"):]
    return repoeval_name


def find_ground_truth_file(original_project_name: str, generated_filename: str, use_project_code: bool = False) -> Optional[Path]:
    """
    æŸ¥æ‰¾å‚è€ƒæ–‡ä»¶
    
    å‚æ•°:
        original_project_name: åŸå§‹é¡¹ç›®åï¼ˆä¸å« repoeval_ å‰ç¼€ï¼‰
        generated_filename: ç”Ÿæˆçš„æ–‡ä»¶å
        use_project_code: æ˜¯å¦ä½¿ç”¨ project_code ç›®å½•ï¼ˆå¼€å…³ï¼‰
            - Falseï¼ˆé»˜è®¤ï¼‰: ä½¿ç”¨ generation_context_ground_truth
            - True: ä½¿ç”¨ project_code/[é¡¹ç›®å]/FUN/
    
    è¿”å›:
        å‚è€ƒæ–‡ä»¶çš„è·¯å¾„ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å› None
    """
    if use_project_code:
        # ä½¿ç”¨ project_code/[é¡¹ç›®å]/FUN/ è·¯å¾„ï¼ˆæ—§é€»è¾‘ï¼‰
        dataset_base = REPO_ROOT / "dataset" / "project_code"
        project_dir = dataset_base / original_project_name
        
        if not project_dir.exists():
            return None
        
        # åœ¨ FUN ç›®å½•ä¸‹æŸ¥æ‰¾æ–‡ä»¶
        fun_dir = project_dir / "FUN"
        if not fun_dir.exists():
            return None
        
        ground_truth_file = fun_dir / generated_filename
        if ground_truth_file.exists():
            return ground_truth_file
    else:
        # ä½¿ç”¨ generation_context_ground_truth è·¯å¾„ï¼ˆæ–°é€»è¾‘ï¼Œé»˜è®¤ï¼‰
        dataset_base = REPO_ROOT / "dataset" / "generation_context_ground_truth"
        project_dir = dataset_base / original_project_name
        
        if not project_dir.exists():
            return None
        
        # ç›´æ¥åœ¨é¡¹ç›®ç›®å½•ä¸‹æŸ¥æ‰¾æ–‡ä»¶
        ground_truth_file = project_dir / generated_filename
        if ground_truth_file.exists():
            return ground_truth_file
    
    return None


def evaluate_project_codebleu(
    project_dir: Path,
    lang: str = "python",
    weights: tuple = (0.25, 0.25, 0.25, 0.25),
    use_project_code: bool = False,
    readful_result_subdir: str = "readful_result"
) -> Optional[Dict]:
    """
    è¯„ä¼°å•ä¸ªé¡¹ç›®çš„ CodeBLEU åˆ†æ•°
    
    å‚æ•°:
        project_dir: é¡¹ç›®ç›®å½•è·¯å¾„ï¼ˆåŒ…å« readful_result/ å­ç›®å½•ï¼‰
        lang: ç¼–ç¨‹è¯­è¨€ï¼ˆé»˜è®¤ pythonï¼Œç”¨äº ST ä»£ç çš„è¿‘ä¼¼è¯„ä¼°ï¼‰
        weights: CodeBLEU å„éƒ¨åˆ†æƒé‡ (ngram, weighted_ngram, syntax, dataflow)
        use_project_code: æ˜¯å¦ä½¿ç”¨ project_code ä½œä¸ºå‚è€ƒï¼ˆé»˜è®¤ Falseï¼‰
            - False: ä½¿ç”¨ generation_context_ground_truthï¼ˆé»˜è®¤ï¼‰
            - True: ä½¿ç”¨ project_code/[é¡¹ç›®å]/FUN/
        readful_result_subdir: è¦è¯„ä¼°çš„ä»£ç å­ç›®å½•åï¼ˆé»˜è®¤ "readful_result"ï¼‰
            - "readful_result": å®Œæ•´ä»£ç ï¼ˆåŒ…å« provide_codeï¼‰
            - "readful_result_no_provide": å»é™¤ provide_code çš„ä»£ç 
    
    è¿”å›:
        åŒ…å«è¯„ä¼°ç»“æœçš„å­—å…¸ï¼Œå¦‚æœè¯„ä¼°å¤±è´¥åˆ™è¿”å› None
    """
    
    # æ£€æŸ¥æŒ‡å®šçš„ä»£ç ç›®å½•æ˜¯å¦å­˜åœ¨
    readful_result_dir = project_dir / readful_result_subdir
    
    if not readful_result_dir.exists():
        print(f"  âš ï¸  è·³è¿‡è¯„ä¼°: æœªæ‰¾åˆ° {readful_result_subdir} ç›®å½•")
        return None
    
    # è·å–åŸå§‹é¡¹ç›®å
    project_name = project_dir.name
    original_project_name = get_original_project_name(project_name)
    
    print(f"  é¡¹ç›®åç§°: {project_name}")
    print(f"  åŸå§‹é¡¹ç›®: {original_project_name}")
    
    try:
        # è·å–æ‰€æœ‰ç”Ÿæˆçš„ .st æ–‡ä»¶
        generated_files = list(readful_result_dir.glob("*.st"))
        
        if not generated_files:
            print(f"  âš ï¸  è­¦å‘Š: readful_result ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ° .st æ–‡ä»¶")
            return None
        
        print(f"  ğŸ“Š æ‰¾åˆ° {len(generated_files)} ä¸ªç”Ÿæˆçš„ .st æ–‡ä»¶ï¼Œå¼€å§‹è¯„ä¼°...")
        
        # é€ä¸ªè¯„ä¼°æ¯ä¸ªæ–‡ä»¶
        case_results = []
        total_codebleu = 0.0
        total_ngram = 0.0
        total_weighted_ngram = 0.0
        total_syntax = 0.0
        total_dataflow = 0.0
        successful_cases = 0
        
        for idx, generated_file in enumerate(sorted(generated_files)):
            filename = generated_file.name
            print(f"\n    [{idx + 1}/{len(generated_files)}] è¯„ä¼° {filename}...", end=' ')
            
            # æŸ¥æ‰¾å¯¹åº”çš„å‚è€ƒæ–‡ä»¶
            ground_truth_file = find_ground_truth_file(original_project_name, filename, use_project_code)
            
            if ground_truth_file is None:
                print(f"âŒ æœªæ‰¾åˆ°å‚è€ƒæ–‡ä»¶")
                case_results.append({
                    'filename': filename,
                    'case_id': idx,
                    'error': 'æœªæ‰¾åˆ°å¯¹åº”çš„å‚è€ƒæ–‡ä»¶'
                })
                continue
            
            # è¯»å–ç”Ÿæˆä»£ç å’Œå‚è€ƒä»£ç 
            try:
                with open(generated_file, 'r', encoding='utf-8') as f:
                    generated_code = f.read()
                
                with open(ground_truth_file, 'r', encoding='utf-8') as f:
                    reference_code = f.read()
                
                # è®¡ç®— CodeBLEU
                result = calc_codebleu(
                    [reference_code],
                    [generated_code],
                    lang=lang,
                    weights=weights,
                    tokenizer=None
                )
                
                case_result = {
                    'filename': filename,
                    'case_id': idx,
                    'ground_truth_path': str(ground_truth_file),
                    'codebleu': result['codebleu'],
                    'ngram_match_score': result['ngram_match_score'],
                    'weighted_ngram_match_score': result['weighted_ngram_match_score'],
                    'syntax_match_score': result['syntax_match_score'],
                    'dataflow_match_score': result['dataflow_match_score'],
                    'reference_length': len(reference_code),
                    'prediction_length': len(generated_code)
                }
                
                case_results.append(case_result)
                
                total_codebleu += result['codebleu']
                total_ngram += result['ngram_match_score']
                total_weighted_ngram += result['weighted_ngram_match_score']
                total_syntax += result['syntax_match_score']
                total_dataflow += result['dataflow_match_score']
                successful_cases += 1
                
                print(f"âœ… CodeBLEU={result['codebleu']:.4f}")
                
            except Exception as e:
                print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
                case_results.append({
                    'filename': filename,
                    'case_id': idx,
                    'error': str(e)
                })
        
        if successful_cases == 0:
            print(f"\n  âš ï¸  è­¦å‘Š: æ²¡æœ‰æˆåŠŸè¯„ä¼°çš„æ–‡ä»¶")
            return None
        
        # è®¡ç®—å¹³å‡å€¼
        avg_codebleu = total_codebleu / successful_cases
        avg_ngram = total_ngram / successful_cases
        avg_weighted_ngram = total_weighted_ngram / successful_cases
        avg_syntax = total_syntax / successful_cases
        avg_dataflow = total_dataflow / successful_cases
        
        # æ„å»ºè¯„ä¼°ç»“æœ
        evaluation_result = {
            'project_name': project_name,
            'original_project_name': original_project_name,
            'total_files': len(generated_files),
            'successful_evaluations': successful_cases,
            'language': lang,
            'weights': weights,
            'average_scores': {
                'codebleu': avg_codebleu,
                'ngram_match_score': avg_ngram,
                'weighted_ngram_match_score': avg_weighted_ngram,
                'syntax_match_score': avg_syntax,
                'dataflow_match_score': avg_dataflow
            },
            'file_results': case_results
        }
        
        print(f"\n  âœ… è¯„ä¼°å®Œæˆ:")
        print(f"     æˆåŠŸè¯„ä¼°: {successful_cases}/{len(generated_files)} ä¸ªæ–‡ä»¶")
        print(f"     å¹³å‡ CodeBLEU: {avg_codebleu:.4f}")
        print(f"     N-gram åŒ¹é…:   {avg_ngram:.4f}")
        print(f"     è¯­æ³•æ ‘åŒ¹é…:    {avg_syntax:.4f}")
        print(f"     æ•°æ®æµåŒ¹é…:    {avg_dataflow:.4f}")
        
        return evaluation_result
        
    except Exception as e:
        print(f"  âŒ è¯„ä¼°å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return None


def save_evaluation_result(evaluation_result: Dict, output_path: Path) -> bool:
    """
    ä¿å­˜è¯„ä¼°ç»“æœåˆ° JSON æ–‡ä»¶
    
    å‚æ•°:
        evaluation_result: è¯„ä¼°ç»“æœå­—å…¸
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    
    è¿”å›:
        æ˜¯å¦ä¿å­˜æˆåŠŸ
    """
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(evaluation_result, f, indent=2, ensure_ascii=False)
        print(f"  ğŸ’¾ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        return True
    except Exception as e:
        print(f"  âŒ ä¿å­˜è¯„ä¼°ç»“æœå¤±è´¥: {e}")
        return False


def evaluate_and_save(
    project_dir: Path,
    output_filename: str = "codebleu_evaluation.json",
    lang: str = "python",
    use_project_code: bool = False,
    readful_result_subdir: str = "readful_result"
) -> bool:
    """
    è¯„ä¼°é¡¹ç›®å¹¶ä¿å­˜ç»“æœ
    
    å‚æ•°:
        project_dir: é¡¹ç›®ç›®å½•
        output_filename: è¾“å‡ºæ–‡ä»¶å
        lang: ç¼–ç¨‹è¯­è¨€
        use_project_code: æ˜¯å¦ä½¿ç”¨ project_code ä½œä¸ºå‚è€ƒï¼ˆé»˜è®¤ Falseï¼‰
            - False: ä½¿ç”¨ generation_context_ground_truthï¼ˆé»˜è®¤ï¼‰
            - True: ä½¿ç”¨ project_code/[é¡¹ç›®å]/FUN/
        readful_result_subdir: è¦è¯„ä¼°çš„ä»£ç å­ç›®å½•åï¼ˆé»˜è®¤ "readful_result"ï¼‰
            - "readful_result": å®Œæ•´ä»£ç ï¼ˆåŒ…å« provide_codeï¼‰
            - "readful_result_no_provide": å»é™¤ provide_code çš„ä»£ç 
    
    è¿”å›:
        æ˜¯å¦æˆåŠŸ
    """
    print(f"\nğŸ“Š å¼€å§‹è¯„ä¼°: {project_dir.name}")
    print(f"{'='*80}")
    
    # æ˜¾ç¤ºå‚è€ƒä»£ç æ¥æºå’Œè¯„æµ‹ä»£ç æ¥æº
    if use_project_code:
        print(f"  å‚è€ƒä»£ç æ¥æº: dataset/project_code/[é¡¹ç›®å]/FUN/")
        print(f"  è¯„æµ‹ä»£ç æ¥æº: {readful_result_subdir}/")
    else:
        print(f"  å‚è€ƒä»£ç æ¥æº: dataset/generation_context_ground_truth/[é¡¹ç›®å]/")
        print(f"  è¯„æµ‹ä»£ç æ¥æº: {readful_result_subdir}/")
    
    # æ‰§è¡Œè¯„ä¼°
    result = evaluate_project_codebleu(
        project_dir, 
        lang=lang, 
        use_project_code=use_project_code,
        readful_result_subdir=readful_result_subdir
    )
    
    if result is None:
        return False
    
    # ä¿å­˜ç»“æœ
    output_path = project_dir / output_filename
    return save_evaluation_result(result, output_path)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    import argparse
    
    parser = argparse.ArgumentParser(description="CodeBLEU è¯„ä¼°å·¥å…·")
    parser.add_argument("project_dir", type=str, help="é¡¹ç›®ç›®å½•è·¯å¾„")
    parser.add_argument("--lang", type=str, default="python", help="ç¼–ç¨‹è¯­è¨€ï¼ˆé»˜è®¤: pythonï¼‰")
    parser.add_argument("--output", type=str, default="codebleu_evaluation.json", help="è¾“å‡ºæ–‡ä»¶å")
    parser.add_argument(
        "--use_project_code_gt",
        action="store_true",
        help="ä½¿ç”¨ project_code/[é¡¹ç›®å]/FUN/ ä½œä¸ºå‚è€ƒä»£ç ï¼ˆé»˜è®¤ä½¿ç”¨ generation_context_ground_truthï¼‰"
    )
    
    args = parser.parse_args()
    
    project_dir = Path(args.project_dir)
    if not project_dir.exists():
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {project_dir}")
        sys.exit(1)
    
    success = evaluate_and_save(project_dir, args.output, args.lang, use_project_code=args.use_project_code_gt)
    sys.exit(0 if success else 1)

