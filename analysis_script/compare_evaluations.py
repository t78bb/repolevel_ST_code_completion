#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ¯”è¾ƒä¸¤ä¸ª evaluation_summary æ–‡ä»¶
å°†æŒ‡å®šæ–‡ä»¶ä¸åŸºå‡†æ–‡ä»¶è¿›è¡Œè¯¦ç»†å¯¹æ¯”ï¼Œè¾“å‡ºä¸º JSON æ ¼å¼
"""

import argparse
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional


# åŸºå‡†æ–‡ä»¶è·¯å¾„ï¼ˆå†™æ­»ï¼‰
BASELINE_FILE = Path(r"D:\graduate_project\é¡¹ç›®çº§stè¡¥å…¨\repo_gen_project\real_groud_truthæœ€æ–°\evaluation_summary_20260121_171642.json")


def load_json(file_path: Path) -> Optional[Dict]:
    """
    åŠ è½½ JSON æ–‡ä»¶
    
    å‚æ•°:
        file_path: JSON æ–‡ä»¶è·¯å¾„
    
    è¿”å›:
        JSON æ•°æ®å­—å…¸ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å› None
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        return None


def calculate_diff(baseline_value: float, compare_value: float) -> Dict[str, float]:
    """
    è®¡ç®—å·®å¼‚
    
    å‚æ•°:
        baseline_value: åŸºå‡†å€¼
        compare_value: å¯¹æ¯”å€¼
    
    è¿”å›:
        åŒ…å«ç»å¯¹å·®å¼‚å’Œç›¸å¯¹å·®å¼‚çš„å­—å…¸
    """
    absolute_diff = compare_value - baseline_value
    
    if baseline_value != 0:
        relative_diff = (absolute_diff / baseline_value) * 100
    else:
        relative_diff = 0.0 if compare_value == 0 else float('inf')
    
    return {
        "baseline": round(baseline_value, 6),
        "compare": round(compare_value, 6),
        "absolute_diff": round(absolute_diff, 6),
        "relative_diff_percent": round(relative_diff, 2)
    }


def compare_overall_statistics(baseline_data: Dict, compare_data: Dict) -> Dict:
    """
    æ¯”è¾ƒæ•´ä½“ç»Ÿè®¡ä¿¡æ¯
    
    å‚æ•°:
        baseline_data: åŸºå‡†æ•°æ®
        compare_data: å¯¹æ¯”æ•°æ®
    
    è¿”å›:
        æ¯”è¾ƒç»“æœå­—å…¸
    """
    result = {
        "total_projects": {
            "baseline": baseline_data.get("total_projects", 0),
            "compare": compare_data.get("total_projects", 0),
            "diff": compare_data.get("total_projects", 0) - baseline_data.get("total_projects", 0)
        },
        "success_count": {
            "baseline": baseline_data.get("success_count", 0),
            "compare": compare_data.get("success_count", 0),
            "diff": compare_data.get("success_count", 0) - baseline_data.get("success_count", 0)
        },
        "failed_count": {
            "baseline": baseline_data.get("failed_count", 0),
            "compare": compare_data.get("failed_count", 0),
            "diff": compare_data.get("failed_count", 0) - baseline_data.get("failed_count", 0)
        }
    }
    
    # æ¯”è¾ƒæ•´ä½“å¹³å‡åˆ†æ•°
    baseline_stats = baseline_data.get("overall_statistics", {})
    compare_stats = compare_data.get("overall_statistics", {})
    
    result["total_files"] = {
        "baseline": baseline_stats.get("total_files", 0),
        "compare": compare_stats.get("total_files", 0),
        "diff": compare_stats.get("total_files", 0) - baseline_stats.get("total_files", 0)
    }
    
    # æ¯”è¾ƒå„é¡¹å¹³å‡åˆ†æ•°
    baseline_scores = baseline_stats.get("average_scores", {})
    compare_scores = compare_stats.get("average_scores", {})
    
    metrics = ["codebleu", "ngram_match_score", "weighted_ngram_match_score", 
               "syntax_match_score", "dataflow_match_score"]
    
    result["average_scores"] = {}
    for metric in metrics:
        baseline_val = baseline_scores.get(metric, 0.0)
        compare_val = compare_scores.get(metric, 0.0)
        result["average_scores"][metric] = calculate_diff(baseline_val, compare_val)
    
    return result


def compare_project_statistics(baseline_data: Dict, compare_data: Dict) -> Dict:
    """
    æ¯”è¾ƒæ¯ä¸ªé¡¹ç›®çš„è¯¦ç»†ç»Ÿè®¡
    
    å‚æ•°:
        baseline_data: åŸºå‡†æ•°æ®
        compare_data: å¯¹æ¯”æ•°æ®
    
    è¿”å›:
        æ¯”è¾ƒç»“æœå­—å…¸
    """
    baseline_projects = baseline_data.get("project_statistics", {})
    compare_projects = compare_data.get("project_statistics", {})
    
    # è·å–æ‰€æœ‰é¡¹ç›®åç§°ï¼ˆå¹¶é›†ï¼‰
    all_projects = set(baseline_projects.keys()) | set(compare_projects.keys())
    
    result = {}
    
    for project_name in sorted(all_projects):
        baseline_proj = baseline_projects.get(project_name)
        compare_proj = compare_projects.get(project_name)
        
        project_result = {}
        
        # é¡¹ç›®å­˜åœ¨æ€§
        if baseline_proj is None:
            project_result["status"] = "æ–°å¢é¡¹ç›®"
            project_result["baseline_exists"] = False
            project_result["compare_exists"] = True
        elif compare_proj is None:
            project_result["status"] = "ç¼ºå¤±é¡¹ç›®"
            project_result["baseline_exists"] = True
            project_result["compare_exists"] = False
        else:
            project_result["status"] = "å…±åŒé¡¹ç›®"
            project_result["baseline_exists"] = True
            project_result["compare_exists"] = True
        
        # æ¯”è¾ƒæ–‡ä»¶æ•°é‡
        if baseline_proj and compare_proj:
            project_result["total_files"] = {
                "baseline": baseline_proj.get("total_files", 0),
                "compare": compare_proj.get("total_files", 0),
                "diff": compare_proj.get("total_files", 0) - baseline_proj.get("total_files", 0)
            }
            
            project_result["successful_evaluations"] = {
                "baseline": baseline_proj.get("successful_evaluations", 0),
                "compare": compare_proj.get("successful_evaluations", 0),
                "diff": compare_proj.get("successful_evaluations", 0) - baseline_proj.get("successful_evaluations", 0)
            }
            
            # æ¯”è¾ƒå„é¡¹åˆ†æ•°
            baseline_scores = baseline_proj.get("average_scores", {})
            compare_scores = compare_proj.get("average_scores", {})
            
            metrics = ["codebleu", "ngram_match_score", "weighted_ngram_match_score", 
                      "syntax_match_score", "dataflow_match_score"]
            
            project_result["average_scores"] = {}
            for metric in metrics:
                baseline_val = baseline_scores.get(metric, 0.0)
                compare_val = compare_scores.get(metric, 0.0)
                project_result["average_scores"][metric] = calculate_diff(baseline_val, compare_val)
        
        elif compare_proj:
            # åªåœ¨å¯¹æ¯”æ•°æ®ä¸­å­˜åœ¨
            project_result["total_files"] = compare_proj.get("total_files", 0)
            project_result["successful_evaluations"] = compare_proj.get("successful_evaluations", 0)
            project_result["average_scores"] = compare_proj.get("average_scores", {})
        
        elif baseline_proj:
            # åªåœ¨åŸºå‡†æ•°æ®ä¸­å­˜åœ¨
            project_result["total_files"] = baseline_proj.get("total_files", 0)
            project_result["successful_evaluations"] = baseline_proj.get("successful_evaluations", 0)
            project_result["average_scores"] = baseline_proj.get("average_scores", {})
        
        result[project_name] = project_result
    
    return result


def compare_project_lists(baseline_data: Dict, compare_data: Dict) -> Dict:
    """
    æ¯”è¾ƒé¡¹ç›®åˆ—è¡¨ï¼ˆæˆåŠŸ/å¤±è´¥/è·³è¿‡ï¼‰
    
    å‚æ•°:
        baseline_data: åŸºå‡†æ•°æ®
        compare_data: å¯¹æ¯”æ•°æ®
    
    è¿”å›:
        æ¯”è¾ƒç»“æœå­—å…¸
    """
    baseline_results = baseline_data.get("results", {})
    compare_results = compare_data.get("results", {})
    
    baseline_success = set(baseline_results.get("success", []))
    compare_success = set(compare_results.get("success", []))
    
    baseline_failed = set(baseline_results.get("failed", []))
    compare_failed = set(compare_results.get("failed", []))
    
    return {
        "success": {
            "baseline_count": len(baseline_success),
            "compare_count": len(compare_success),
            "common": sorted(list(baseline_success & compare_success)),
            "only_in_baseline": sorted(list(baseline_success - compare_success)),
            "only_in_compare": sorted(list(compare_success - baseline_success)),
            "newly_successful": sorted(list((compare_success - baseline_success) & baseline_failed)),  # ä»å¤±è´¥å˜æˆåŠŸ
            "newly_failed": sorted(list((baseline_success - compare_success) & compare_failed))  # ä»æˆåŠŸå˜å¤±è´¥
        },
        "failed": {
            "baseline_count": len(baseline_failed),
            "compare_count": len(compare_failed),
            "common": sorted(list(baseline_failed & compare_failed)),
            "only_in_baseline": sorted(list(baseline_failed - compare_failed)),
            "only_in_compare": sorted(list(compare_failed - baseline_failed))
        }
    }


def generate_summary(comparison_result: Dict) -> Dict:
    """
    ç”Ÿæˆæ¯”è¾ƒæ€»ç»“
    
    å‚æ•°:
        comparison_result: å®Œæ•´çš„æ¯”è¾ƒç»“æœ
    
    è¿”å›:
        æ€»ç»“å­—å…¸
    """
    overall = comparison_result.get("overall_comparison", {})
    project_lists = comparison_result.get("project_lists_comparison", {})
    
    summary = {
        "total_projects_change": overall.get("total_projects", {}).get("diff", 0),
        "success_count_change": overall.get("success_count", {}).get("diff", 0),
        "failed_count_change": overall.get("failed_count", {}).get("diff", 0),
        "newly_successful_projects": len(project_lists.get("success", {}).get("newly_successful", [])),
        "newly_failed_projects": len(project_lists.get("success", {}).get("newly_failed", [])),
        "codebleu_change": overall.get("average_scores", {}).get("codebleu", {}).get("relative_diff_percent", 0.0)
    }
    
    # è®¡ç®—æ”¹è¿›/ä¸‹é™çš„é¡¹ç›®æ•°é‡
    project_stats = comparison_result.get("project_statistics_comparison", {})
    improved = 0
    degraded = 0
    unchanged = 0
    
    for project_name, project_data in project_stats.items():
        if project_data.get("status") == "å…±åŒé¡¹ç›®":
            codebleu_diff = project_data.get("average_scores", {}).get("codebleu", {}).get("absolute_diff", 0.0)
            if codebleu_diff > 0.01:  # æå‡è¶…è¿‡ 1%
                improved += 1
            elif codebleu_diff < -0.01:  # ä¸‹é™è¶…è¿‡ 1%
                degraded += 1
            else:
                unchanged += 1
    
    summary["improved_projects"] = improved
    summary["degraded_projects"] = degraded
    summary["unchanged_projects"] = unchanged
    
    return summary


def main():
    parser = argparse.ArgumentParser(description="æ¯”è¾ƒä¸¤ä¸ª evaluation_summary æ–‡ä»¶")
    parser.add_argument(
        "--file",
        type=str,
        required=True,
        help="è¦æ¯”è¾ƒçš„ evaluation_summary æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="è¾“å‡º JSON æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ï¼šcomparison_result_[timestamp].jsonï¼‰"
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥åŸºå‡†æ–‡ä»¶
    if not BASELINE_FILE.exists():
        print(f"âŒ åŸºå‡†æ–‡ä»¶ä¸å­˜åœ¨: {BASELINE_FILE}")
        return 1
    
    # æ£€æŸ¥å¯¹æ¯”æ–‡ä»¶
    compare_file = Path(args.file)
    if not compare_file.exists():
        print(f"âŒ å¯¹æ¯”æ–‡ä»¶ä¸å­˜åœ¨: {compare_file}")
        return 1
    
    print("="*80)
    print("è¯„ä¼°ç»“æœæ¯”è¾ƒå·¥å…·")
    print("="*80)
    print(f"åŸºå‡†æ–‡ä»¶: {BASELINE_FILE}")
    print(f"å¯¹æ¯”æ–‡ä»¶: {compare_file}")
    print("="*80)
    
    # åŠ è½½æ•°æ®
    print("\nğŸ“‚ åŠ è½½æ•°æ®...")
    baseline_data = load_json(BASELINE_FILE)
    compare_data = load_json(compare_file)
    
    if baseline_data is None or compare_data is None:
        print("âŒ æ•°æ®åŠ è½½å¤±è´¥")
        return 1
    
    print("âœ“ æ•°æ®åŠ è½½æˆåŠŸ")
    
    # æ‰§è¡Œæ¯”è¾ƒ
    print("\nğŸ“Š å¼€å§‹æ¯”è¾ƒ...")
    
    comparison_result = {
        "metadata": {
            "comparison_timestamp": datetime.now().isoformat(),
            "baseline_file": str(BASELINE_FILE),
            "compare_file": str(compare_file),
            "baseline_timestamp": baseline_data.get("timestamp"),
            "compare_timestamp": compare_data.get("timestamp")
        },
        "overall_comparison": compare_overall_statistics(baseline_data, compare_data),
        "project_lists_comparison": compare_project_lists(baseline_data, compare_data),
        "project_statistics_comparison": compare_project_statistics(baseline_data, compare_data)
    }
    
    # ç”Ÿæˆæ€»ç»“
    comparison_result["summary"] = generate_summary(comparison_result)
    
    print("âœ“ æ¯”è¾ƒå®Œæˆ")
    
    # ä¿å­˜ç»“æœ
    if args.output:
        output_file = Path(args.output)
    else:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = Path(__file__).parent / f"comparison_result_{timestamp}.json"
    
    print(f"\nğŸ’¾ ä¿å­˜ç»“æœåˆ°: {output_file}")
    
    try:
        output_file.parent.mkdir(parents=True, exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(comparison_result, f, indent=2, ensure_ascii=False)
        print("âœ“ ä¿å­˜æˆåŠŸ")
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
        return 1
    
    # æ‰“å°æ€»ç»“
    summary = comparison_result["summary"]
    print("\n" + "="*80)
    print("æ¯”è¾ƒæ€»ç»“")
    print("="*80)
    print(f"æ€»é¡¹ç›®æ•°å˜åŒ–:     {summary['total_projects_change']:+d}")
    print(f"æˆåŠŸé¡¹ç›®å˜åŒ–:     {summary['success_count_change']:+d}")
    print(f"å¤±è´¥é¡¹ç›®å˜åŒ–:     {summary['failed_count_change']:+d}")
    print(f"æ–°æˆåŠŸçš„é¡¹ç›®:     {summary['newly_successful_projects']}")
    print(f"æ–°å¤±è´¥çš„é¡¹ç›®:     {summary['newly_failed_projects']}")
    print(f"æ”¹è¿›çš„é¡¹ç›®:       {summary['improved_projects']}")
    print(f"ä¸‹é™çš„é¡¹ç›®:       {summary['degraded_projects']}")
    print(f"ä¸å˜çš„é¡¹ç›®:       {summary['unchanged_projects']}")
    print(f"CodeBLEU å˜åŒ–:    {summary['codebleu_change']:+.2f}%")
    print("="*80)
    
    return 0


if __name__ == "__main__":
    exit(main())

