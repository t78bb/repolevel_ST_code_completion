{
  "overall_statistics": {
    "num_projects": 29,
    "total_cases": 101,
    "overall_codebleu": 0.5945057071370872,
    "overall_edit_distance": 980.7722772277227,
    "overall_edit_similarity": 0.39457087380696154
  },
  "project_results": [
    {
      "project_name": "repoeval_Builder_Application_RPI",
      "num_cases": 5,
      "average_codebleu": 0.5756127028596916,
      "average_edit_distance": 2530.8,
      "average_edit_similarity": 0.21718440023447178,
      "case_results": [
        {
          "codebleu": 0.49461057401636516,
          "codebleu_details": {
            "codebleu": 0.49461057401636516,
            "ngram_match_score": 0.07043314769800961,
            "weighted_ngram_match_score": 0.3880091483674509,
            "syntax_match_score": 0.72,
            "dataflow_match_score": 0.8
          },
          "edit_distance": 2806,
          "normalized_edit_distance": 0.8477341389728097,
          "edit_similarity": 0.1522658610271903,
          "reference_length": 595,
          "prediction_length": 3310
        },
        {
          "codebleu": 0.7301190475239512,
          "codebleu_details": {
            "codebleu": 0.7301190475239512,
            "ngram_match_score": 0.26777114381957073,
            "weighted_ngram_match_score": 0.8410339587430775,
            "syntax_match_score": 0.8461538461538461,
            "dataflow_match_score": 0.9655172413793104
          },
          "edit_distance": 1914,
          "normalized_edit_distance": 0.7155140186915888,
          "edit_similarity": 0.2844859813084112,
          "reference_length": 766,
          "prediction_length": 2675
        },
        {
          "codebleu": 0.4751183579863994,
          "codebleu_details": {
            "codebleu": 0.4751183579863994,
            "ngram_match_score": 0.21786674068336315,
            "weighted_ngram_match_score": 0.5713855691500233,
            "syntax_match_score": 0.5445544554455446,
            "dataflow_match_score": 0.5666666666666667
          },
          "edit_distance": 1680,
          "normalized_edit_distance": 0.6656101426307448,
          "edit_similarity": 0.3343898573692552,
          "reference_length": 1246,
          "prediction_length": 2524
        },
        {
          "codebleu": 0.7463634487186415,
          "codebleu_details": {
            "codebleu": 0.7463634487186415,
            "ngram_match_score": 0.16735464330635677,
            "weighted_ngram_match_score": 0.9903680591312343,
            "syntax_match_score": 0.9705882352941176,
            "dataflow_match_score": 0.8571428571428571
          },
          "edit_distance": 3095,
          "normalized_edit_distance": 0.8602001111728739,
          "edit_similarity": 0.13979988882712613,
          "reference_length": 503,
          "prediction_length": 3598
        },
        {
          "codebleu": 0.4318520860531009,
          "codebleu_details": {
            "codebleu": 0.4318520860531009,
            "ngram_match_score": 0.11074322398263703,
            "weighted_ngram_match_score": 0.39358819715284354,
            "syntax_match_score": 0.5,
            "dataflow_match_score": 0.7230769230769231
          },
          "edit_distance": 3159,
          "normalized_edit_distance": 0.8250195873596239,
          "edit_similarity": 0.17498041264037612,
          "reference_length": 848,
          "prediction_length": 3829
        }
      ]
    },
    {
      "project_name": "repoeval_Command1",
      "num_cases": 12,
      "average_codebleu": 0.7238394802869235,
      "average_edit_distance": 599.1666666666666,
      "average_edit_similarity": 0.20713452440305935,
      "case_results": [
        {
          "codebleu": 0.747888492894399,
          "codebleu_details": {
            "codebleu": 0.747888492894399,
            "ngram_match_score": 0.26821279296640554,
            "weighted_ngram_match_score": 0.8344522897223012,
            "syntax_match_score": 0.8888888888888888,
            "dataflow_match_score": 0
          },
          "edit_distance": 280,
          "normalized_edit_distance": 0.6796116504854369,
          "edit_similarity": 0.3203883495145631,
          "reference_length": 133,
          "prediction_length": 412
        },
        {
          "codebleu": 0.7027255066987813,
          "codebleu_details": {
            "codebleu": 0.7027255066987813,
            "ngram_match_score": 0.08756084818393503,
            "weighted_ngram_match_score": 0.8344522897223012,
            "syntax_match_score": 0.8888888888888888,
            "dataflow_match_score": 0
          },
          "edit_distance": 761,
          "normalized_edit_distance": 0.8521836506159015,
          "edit_similarity": 0.1478163493840985,
          "reference_length": 133,
          "prediction_length": 893
        },
        {
          "codebleu": 0.741523329443784,
          "codebleu_details": {
            "codebleu": 0.741523329443784,
            "ngram_match_score": 0.05700240868422677,
            "weighted_ngram_match_score": 1.0,
            "syntax_match_score": 0.9090909090909091,
            "dataflow_match_score": 0
          },
          "edit_distance": 1535,
          "normalized_edit_distance": 0.9376908979841173,
          "edit_similarity": 0.06230910201588269,
          "reference_length": 102,
          "prediction_length": 1637
        },
        {
          "codebleu": 0.7575516094184684,
          "codebleu_details": {
            "codebleu": 0.7575516094184684,
            "ngram_match_score": 0.10163500910244527,
            "weighted_ngram_match_score": 1.0,
            "syntax_match_score": 0.9285714285714286,
            "dataflow_match_score": 0
          },
          "edit_distance": 826,
          "normalized_edit_distance": 0.8768577494692145,
          "edit_similarity": 0.12314225053078554,
          "reference_length": 116,
          "prediction_length": 942
        },
        {
          "codebleu": 0.7381523022909151,
          "codebleu_details": {
            "codebleu": 0.7381523022909151,
            "ngram_match_score": 0.07760920916366017,
            "weighted_ngram_match_score": 1.0,
            "syntax_match_score": 0.875,
            "dataflow_match_score": 0
          },
          "edit_distance": 550,
          "normalized_edit_distance": 0.8914100486223663,
          "edit_similarity": 0.10858995137763372,
          "reference_length": 67,
          "prediction_length": 617
        },
        {
          "codebleu": 0.6724116213029918,
          "codebleu_details": {
            "codebleu": 0.6724116213029918,
            "ngram_match_score": 0.10301806500059184,
            "weighted_ngram_match_score": 0.669961753544709,
            "syntax_match_score": 0.9166666666666666,
            "dataflow_match_score": 0
          },
          "edit_distance": 504,
          "normalized_edit_distance": 0.8012718600953895,
          "edit_similarity": 0.19872813990461047,
          "reference_length": 131,
          "prediction_length": 629
        },
        {
          "codebleu": 0.7573130913404417,
          "codebleu_details": {
            "codebleu": 0.7573130913404417,
            "ngram_match_score": 0.30591118675057666,
            "weighted_ngram_match_score": 0.8344522897223012,
            "syntax_match_score": 0.8888888888888888,
            "dataflow_match_score": 0
          },
          "edit_distance": 237,
          "normalized_edit_distance": 0.6752136752136753,
          "edit_similarity": 0.32478632478632474,
          "reference_length": 115,
          "prediction_length": 351
        },
        {
          "codebleu": 0.712634254768307,
          "codebleu_details": {
            "codebleu": 0.712634254768307,
            "ngram_match_score": 0.127195840462038,
            "weighted_ngram_match_score": 0.8344522897223012,
            "syntax_match_score": 0.8888888888888888,
            "dataflow_match_score": 0
          },
          "edit_distance": 410,
          "normalized_edit_distance": 0.7824427480916031,
          "edit_similarity": 0.21755725190839692,
          "reference_length": 115,
          "prediction_length": 524
        },
        {
          "codebleu": 0.7424650008023813,
          "codebleu_details": {
            "codebleu": 0.7424650008023813,
            "ngram_match_score": 0.09486000320952537,
            "weighted_ngram_match_score": 1.0,
            "syntax_match_score": 0.875,
            "dataflow_match_score": 0
          },
          "edit_distance": 575,
          "normalized_edit_distance": 0.8942457231726283,
          "edit_similarity": 0.10575427682737171,
          "reference_length": 68,
          "prediction_length": 643
        },
        {
          "codebleu": 0.7054467343450339,
          "codebleu_details": {
            "codebleu": 0.7054467343450339,
            "ngram_match_score": 0.09844575876894558,
            "weighted_ngram_match_score": 0.8344522897223012,
            "syntax_match_score": 0.8888888888888888,
            "dataflow_match_score": 0
          },
          "edit_distance": 618,
          "normalized_edit_distance": 0.8489010989010989,
          "edit_similarity": 0.15109890109890112,
          "reference_length": 111,
          "prediction_length": 728
        },
        {
          "codebleu": 0.763094941652463,
          "codebleu_details": {
            "codebleu": 0.763094941652463,
            "ngram_match_score": 0.3290385879986622,
            "weighted_ngram_match_score": 0.8344522897223012,
            "syntax_match_score": 0.8888888888888888,
            "dataflow_match_score": 0
          },
          "edit_distance": 153,
          "normalized_edit_distance": 0.583969465648855,
          "edit_similarity": 0.416030534351145,
          "reference_length": 110,
          "prediction_length": 262
        },
        {
          "codebleu": 0.6448668784851145,
          "codebleu_details": {
            "codebleu": 0.6448668784851145,
            "ngram_match_score": 0.1979800933209771,
            "weighted_ngram_match_score": 0.7029159920480521,
            "syntax_match_score": 0.8214285714285714,
            "dataflow_match_score": 0.8571428571428571
          },
          "edit_distance": 741,
          "normalized_edit_distance": 0.6905871388630009,
          "edit_similarity": 0.30941286113699906,
          "reference_length": 385,
          "prediction_length": 1073
        }
      ]
    },
    {
      "project_name": "repoeval_Decorator_Application",
      "num_cases": 8,
      "average_codebleu": 0.708854435504475,
      "average_edit_distance": 738.25,
      "average_edit_similarity": 0.4221859183471513,
      "case_results": [
        {
          "codebleu": 0.6850959751239043,
          "codebleu_details": {
            "codebleu": 0.6850959751239043,
            "ngram_match_score": 0.21422628859810763,
            "weighted_ngram_match_score": 0.8821358727670746,
            "syntax_match_score": 0.6875,
            "dataflow_match_score": 0.9565217391304348
          },
          "edit_distance": 1082,
          "normalized_edit_distance": 0.7426218256691832,
          "edit_similarity": 0.2573781743308168,
          "reference_length": 379,
          "prediction_length": 1457
        },
        {
          "codebleu": 0.574033401436134,
          "codebleu_details": {
            "codebleu": 0.574033401436134,
            "ngram_match_score": 0.164688951624263,
            "weighted_ngram_match_score": 0.39334941602503515,
            "syntax_match_score": 0.8214285714285714,
            "dataflow_match_score": 0.9166666666666666
          },
          "edit_distance": 610,
          "normalized_edit_distance": 0.6900452488687783,
          "edit_similarity": 0.30995475113122173,
          "reference_length": 341,
          "prediction_length": 884
        },
        {
          "codebleu": 0.5384849695021605,
          "codebleu_details": {
            "codebleu": 0.5384849695021605,
            "ngram_match_score": 0.1354089639177313,
            "weighted_ngram_match_score": 0.3615612171212138,
            "syntax_match_score": 0.696969696969697,
            "dataflow_match_score": 0.96
          },
          "edit_distance": 725,
          "normalized_edit_distance": 0.7045675413022352,
          "edit_similarity": 0.2954324586977648,
          "reference_length": 390,
          "prediction_length": 1029
        },
        {
          "codebleu": 0.748491864314433,
          "codebleu_details": {
            "codebleu": 0.748491864314433,
            "ngram_match_score": 0.20261945706506077,
            "weighted_ngram_match_score": 0.9580146668593382,
            "syntax_match_score": 0.9333333333333333,
            "dataflow_match_score": 0.9
          },
          "edit_distance": 1019,
          "normalized_edit_distance": 0.7531411677753141,
          "edit_similarity": 0.24685883222468585,
          "reference_length": 334,
          "prediction_length": 1353
        },
        {
          "codebleu": 0.781392366911856,
          "codebleu_details": {
            "codebleu": 0.781392366911856,
            "ngram_match_score": 0.707889570549183,
            "weighted_ngram_match_score": 0.7854959890522639,
            "syntax_match_score": 0.6666666666666666,
            "dataflow_match_score": 0.9655172413793104
          },
          "edit_distance": 51,
          "normalized_edit_distance": 0.09807692307692308,
          "edit_similarity": 0.9019230769230769,
          "reference_length": 504,
          "prediction_length": 520
        },
        {
          "codebleu": 0.8293627092164259,
          "codebleu_details": {
            "codebleu": 0.8293627092164259,
            "ngram_match_score": 0.7605681651018249,
            "weighted_ngram_match_score": 0.7791048939861007,
            "syntax_match_score": 0.7777777777777778,
            "dataflow_match_score": 1.0
          },
          "edit_distance": 31,
          "normalized_edit_distance": 0.07126436781609195,
          "edit_similarity": 0.9287356321839081,
          "reference_length": 418,
          "prediction_length": 435
        },
        {
          "codebleu": 0.7709799550593545,
          "codebleu_details": {
            "codebleu": 0.7709799550593545,
            "ngram_match_score": 0.19257182004474627,
            "weighted_ngram_match_score": 0.9580146668593382,
            "syntax_match_score": 0.9333333333333333,
            "dataflow_match_score": 1.0
          },
          "edit_distance": 1207,
          "normalized_edit_distance": 0.7832576249188838,
          "edit_similarity": 0.21674237508111616,
          "reference_length": 334,
          "prediction_length": 1541
        },
        {
          "codebleu": 0.7429942424715316,
          "codebleu_details": {
            "codebleu": 0.7429942424715316,
            "ngram_match_score": 0.1806289696934547,
            "weighted_ngram_match_score": 0.9580146668593382,
            "syntax_match_score": 0.9333333333333333,
            "dataflow_match_score": 0.9
          },
          "edit_distance": 1181,
          "normalized_edit_distance": 0.7795379537953795,
          "edit_similarity": 0.2204620462046205,
          "reference_length": 334,
          "prediction_length": 1515
        }
      ]
    },
    {
      "project_name": "repoeval_FB50_虚轴控制",
      "num_cases": 3,
      "average_codebleu": 0.551464324155738,
      "average_edit_distance": 1617.3333333333333,
      "average_edit_similarity": 0.2946968486951842,
      "case_results": [
        {
          "codebleu": 0.5026992123385353,
          "codebleu_details": {
            "codebleu": 0.5026992123385353,
            "ngram_match_score": 0.018854616662320772,
            "weighted_ngram_match_score": 0.2551001274286626,
            "syntax_match_score": 0.7368421052631579,
            "dataflow_match_score": 1.0
          },
          "edit_distance": 1899,
          "normalized_edit_distance": 0.9263414634146342,
          "edit_similarity": 0.07365853658536581,
          "reference_length": 180,
          "prediction_length": 2050
        },
        {
          "codebleu": 0.4830889386806095,
          "codebleu_details": {
            "codebleu": 0.4830889386806095,
            "ngram_match_score": 0.2289657156514136,
            "weighted_ngram_match_score": 0.2639671093297109,
            "syntax_match_score": 0.8351063829787234,
            "dataflow_match_score": 0.60431654676259
          },
          "edit_distance": 2678,
          "normalized_edit_distance": 0.6283435007038949,
          "edit_similarity": 0.3716564992961051,
          "reference_length": 4262,
          "prediction_length": 4196
        },
        {
          "codebleu": 0.668604821448069,
          "codebleu_details": {
            "codebleu": 0.668604821448069,
            "ngram_match_score": 0.2504037387704623,
            "weighted_ngram_match_score": 0.5979285905000744,
            "syntax_match_score": 0.8260869565217391,
            "dataflow_match_score": 1.0
          },
          "edit_distance": 275,
          "normalized_edit_distance": 0.5612244897959183,
          "edit_similarity": 0.4387755102040817,
          "reference_length": 232,
          "prediction_length": 490
        }
      ]
    },
    {
      "project_name": "repoeval_GreatExampleOfAdvantages",
      "num_cases": 2,
      "average_codebleu": 0.43019183535615424,
      "average_edit_distance": 862.0,
      "average_edit_similarity": 0.2017647989870212,
      "case_results": [
        {
          "codebleu": 0.5365111981250277,
          "codebleu_details": {
            "codebleu": 0.5365111981250277,
            "ngram_match_score": 0.05293674746467827,
            "weighted_ngram_match_score": 0.47406042598781367,
            "syntax_match_score": 0.9523809523809523,
            "dataflow_match_score": 0.6666666666666666
          },
          "edit_distance": 602,
          "normalized_edit_distance": 0.8269230769230769,
          "edit_similarity": 0.17307692307692313,
          "reference_length": 142,
          "prediction_length": 728
        },
        {
          "codebleu": 0.3238724725872808,
          "codebleu_details": {
            "codebleu": 0.3238724725872808,
            "ngram_match_score": 0.06339220531655786,
            "weighted_ngram_match_score": 0.06344689138177172,
            "syntax_match_score": 0.6130952380952381,
            "dataflow_match_score": 0.5555555555555556
          },
          "edit_distance": 1122,
          "normalized_edit_distance": 0.7695473251028807,
          "edit_similarity": 0.2304526748971193,
          "reference_length": 1458,
          "prediction_length": 954
        }
      ]
    },
    {
      "project_name": "repoeval_Interation_HowTo",
      "num_cases": 2,
      "average_codebleu": 0.5587764027325024,
      "average_edit_distance": 392.5,
      "average_edit_similarity": 0.40855813953488374,
      "case_results": [
        {
          "codebleu": 0.5862201517563073,
          "codebleu_details": {
            "codebleu": 0.5862201517563073,
            "ngram_match_score": 0.18759690172571009,
            "weighted_ngram_match_score": 0.4498930567323998,
            "syntax_match_score": 0.7843137254901961,
            "dataflow_match_score": 0.9230769230769231
          },
          "edit_distance": 486,
          "normalized_edit_distance": 0.648,
          "edit_similarity": 0.352,
          "reference_length": 455,
          "prediction_length": 750
        },
        {
          "codebleu": 0.5313326537086975,
          "codebleu_details": {
            "codebleu": 0.5313326537086975,
            "ngram_match_score": 0.2828253094465425,
            "weighted_ngram_match_score": 0.3195851016531034,
            "syntax_match_score": 0.8387096774193549,
            "dataflow_match_score": 0.6842105263157895
          },
          "edit_distance": 299,
          "normalized_edit_distance": 0.5348837209302325,
          "edit_similarity": 0.4651162790697675,
          "reference_length": 559,
          "prediction_length": 490
        }
      ]
    },
    {
      "project_name": "repoeval_PID_controller",
      "num_cases": 4,
      "average_codebleu": 0.6178005511468445,
      "average_edit_distance": 911.0,
      "average_edit_similarity": 0.5622591455470596,
      "case_results": [
        {
          "codebleu": 0.679626341707212,
          "codebleu_details": {
            "codebleu": 0.679626341707212,
            "ngram_match_score": 0.4630966343619595,
            "weighted_ngram_match_score": 0.6147837324668887,
            "syntax_match_score": 0.765625,
            "dataflow_match_score": 0.875
          },
          "edit_distance": 416,
          "normalized_edit_distance": 0.36651982378854625,
          "edit_similarity": 0.6334801762114537,
          "reference_length": 844,
          "prediction_length": 1135
        },
        {
          "codebleu": 0.6533526407361742,
          "codebleu_details": {
            "codebleu": 0.6533526407361742,
            "ngram_match_score": 0.5598956050782635,
            "weighted_ngram_match_score": 0.5860576523635865,
            "syntax_match_score": 0.864516129032258,
            "dataflow_match_score": 0.6029411764705882
          },
          "edit_distance": 413,
          "normalized_edit_distance": 0.30345334313005146,
          "edit_similarity": 0.6965466568699485,
          "reference_length": 1361,
          "prediction_length": 992
        },
        {
          "codebleu": 0.5675316015253964,
          "codebleu_details": {
            "codebleu": 0.5675316015253964,
            "ngram_match_score": 0.23254091113779216,
            "weighted_ngram_match_score": 0.5902752517803002,
            "syntax_match_score": 0.6864406779661016,
            "dataflow_match_score": 0.7608695652173914
          },
          "edit_distance": 2055,
          "normalized_edit_distance": 0.7120582120582121,
          "edit_similarity": 0.28794178794178793,
          "reference_length": 1017,
          "prediction_length": 2886
        },
        {
          "codebleu": 0.5706916206185957,
          "codebleu_details": {
            "codebleu": 0.5706916206185957,
            "ngram_match_score": 0.4739194612918753,
            "weighted_ngram_match_score": 0.47447741841912233,
            "syntax_match_score": 0.7333333333333333,
            "dataflow_match_score": 0.6010362694300518
          },
          "edit_distance": 760,
          "normalized_edit_distance": 0.36893203883495146,
          "edit_similarity": 0.6310679611650485,
          "reference_length": 2060,
          "prediction_length": 1876
        }
      ]
    },
    {
      "project_name": "repoeval_PT1Filter",
      "num_cases": 1,
      "average_codebleu": 0.7035594749657302,
      "average_edit_distance": 346.0,
      "average_edit_similarity": 0.6099210822998873,
      "case_results": [
        {
          "codebleu": 0.7035594749657302,
          "codebleu_details": {
            "codebleu": 0.7035594749657302,
            "ngram_match_score": 0.47928377157135743,
            "weighted_ngram_match_score": 0.6401489334863689,
            "syntax_match_score": 0.9090909090909091,
            "dataflow_match_score": 0.7857142857142857
          },
          "edit_distance": 346,
          "normalized_edit_distance": 0.3900789177001127,
          "edit_similarity": 0.6099210822998873,
          "reference_length": 587,
          "prediction_length": 887
        }
      ]
    },
    {
      "project_name": "repoeval_ProductionLine",
      "num_cases": 6,
      "average_codebleu": 0.7218519695232356,
      "average_edit_distance": 530.0,
      "average_edit_similarity": 0.4709162441543229,
      "case_results": [
        {
          "codebleu": 0.67644901512268,
          "codebleu_details": {
            "codebleu": 0.67644901512268,
            "ngram_match_score": 0.049735368909879014,
            "weighted_ngram_match_score": 0.6930977286178778,
            "syntax_match_score": 0.9629629629629629,
            "dataflow_match_score": 1.0
          },
          "edit_distance": 1525,
          "normalized_edit_distance": 0.8975868157739847,
          "edit_similarity": 0.10241318422601531,
          "reference_length": 174,
          "prediction_length": 1699
        },
        {
          "codebleu": 0.8705528661935331,
          "codebleu_details": {
            "codebleu": 0.8705528661935331,
            "ngram_match_score": 0.6836013949954282,
            "weighted_ngram_match_score": 0.8242510954197301,
            "syntax_match_score": 0.9743589743589743,
            "dataflow_match_score": 1.0
          },
          "edit_distance": 12,
          "normalized_edit_distance": 0.043010752688172046,
          "edit_similarity": 0.956989247311828,
          "reference_length": 267,
          "prediction_length": 279
        },
        {
          "codebleu": 0.6392621556112044,
          "codebleu_details": {
            "codebleu": 0.6392621556112044,
            "ngram_match_score": 0.15036398057738412,
            "weighted_ngram_match_score": 0.769429739906649,
            "syntax_match_score": 0.9705882352941176,
            "dataflow_match_score": 0.6666666666666666
          },
          "edit_distance": 477,
          "normalized_edit_distance": 0.6853448275862069,
          "edit_similarity": 0.31465517241379315,
          "reference_length": 219,
          "prediction_length": 696
        },
        {
          "codebleu": 0.6938004534431138,
          "codebleu_details": {
            "codebleu": 0.6938004534431138,
            "ngram_match_score": 0.10110522773583652,
            "weighted_ngram_match_score": 0.7098108717509044,
            "syntax_match_score": 0.9642857142857143,
            "dataflow_match_score": 1.0
          },
          "edit_distance": 543,
          "normalized_edit_distance": 0.724,
          "edit_similarity": 0.276,
          "reference_length": 211,
          "prediction_length": 750
        },
        {
          "codebleu": 0.6305939733732315,
          "codebleu_details": {
            "codebleu": 0.6305939733732315,
            "ngram_match_score": 0.0924259434787966,
            "weighted_ngram_match_score": 0.8003203203844999,
            "syntax_match_score": 0.9629629629629629,
            "dataflow_match_score": 0.6666666666666666
          },
          "edit_distance": 610,
          "normalized_edit_distance": 0.7840616966580977,
          "edit_similarity": 0.21593830334190234,
          "reference_length": 168,
          "prediction_length": 778
        },
        {
          "codebleu": 0.8204533533956512,
          "codebleu_details": {
            "codebleu": 0.8204533533956512,
            "ngram_match_score": 0.5973854416921761,
            "weighted_ngram_match_score": 0.7061671023252111,
            "syntax_match_score": 0.9782608695652174,
            "dataflow_match_score": 1.0
          },
          "edit_distance": 13,
          "normalized_edit_distance": 0.040498442367601244,
          "edit_similarity": 0.9595015576323987,
          "reference_length": 310,
          "prediction_length": 321
        }
      ]
    },
    {
      "project_name": "repoeval_Proxy_Application",
      "num_cases": 2,
      "average_codebleu": 0.6384906285761354,
      "average_edit_distance": 2929.0,
      "average_edit_similarity": 0.12089015514641355,
      "case_results": [
        {
          "codebleu": 0.666634483471236,
          "codebleu_details": {
            "codebleu": 0.666634483471236,
            "ngram_match_score": 0.1485394926557033,
            "weighted_ngram_match_score": 0.8652206634514629,
            "syntax_match_score": 0.7777777777777778,
            "dataflow_match_score": 0.875
          },
          "edit_distance": 1800,
          "normalized_edit_distance": 0.8024966562639322,
          "edit_similarity": 0.19750334373606782,
          "reference_length": 449,
          "prediction_length": 2243
        },
        {
          "codebleu": 0.6103467736810348,
          "codebleu_details": {
            "codebleu": 0.6103467736810348,
            "ngram_match_score": 0.018828608507960778,
            "weighted_ngram_match_score": 0.5804532230582841,
            "syntax_match_score": 0.8421052631578947,
            "dataflow_match_score": 1.0
          },
          "edit_distance": 4058,
          "normalized_edit_distance": 0.9557230334432407,
          "edit_similarity": 0.04427696655675928,
          "reference_length": 190,
          "prediction_length": 4246
        }
      ]
    },
    {
      "project_name": "repoeval_Robotics_DynamicModel",
      "num_cases": 2,
      "average_codebleu": 0.8126268305415645,
      "average_edit_distance": 1009.5,
      "average_edit_similarity": 0.5813309013290817,
      "case_results": [
        {
          "codebleu": 0.696105107492958,
          "codebleu_details": {
            "codebleu": 0.696105107492958,
            "ngram_match_score": 0.39044628135364523,
            "weighted_ngram_match_score": 0.9539741486181865,
            "syntax_match_score": 0.94,
            "dataflow_match_score": 0.5
          },
          "edit_distance": 1823,
          "normalized_edit_distance": 0.6871466264606106,
          "edit_similarity": 0.31285337353938936,
          "reference_length": 957,
          "prediction_length": 2653
        },
        {
          "codebleu": 0.9291485535901709,
          "codebleu_details": {
            "codebleu": 0.9291485535901709,
            "ngram_match_score": 0.8585167591635382,
            "weighted_ngram_match_score": 0.8847441218638121,
            "syntax_match_score": 0.9733333333333334,
            "dataflow_match_score": 1.0
          },
          "edit_distance": 196,
          "normalized_edit_distance": 0.15019157088122606,
          "edit_similarity": 0.849808429118774,
          "reference_length": 1131,
          "prediction_length": 1305
        }
      ]
    },
    {
      "project_name": "repoeval_Wrappers",
      "num_cases": 1,
      "average_codebleu": 0.46667044714568107,
      "average_edit_distance": 448.0,
      "average_edit_similarity": 0.34883720930232553,
      "case_results": [
        {
          "codebleu": 0.46667044714568107,
          "codebleu_details": {
            "codebleu": 0.46667044714568107,
            "ngram_match_score": 0.18258827606752262,
            "weighted_ngram_match_score": 0.4422567778213243,
            "syntax_match_score": 0.5918367346938775,
            "dataflow_match_score": 0.65
          },
          "edit_distance": 448,
          "normalized_edit_distance": 0.6511627906976745,
          "edit_similarity": 0.34883720930232553,
          "reference_length": 370,
          "prediction_length": 688
        }
      ]
    },
    {
      "project_name": "repoeval__assembly-station_",
      "num_cases": 1,
      "average_codebleu": 0.30610854780951674,
      "average_edit_distance": 1376.0,
      "average_edit_similarity": 0.28778467908902694,
      "case_results": [
        {
          "codebleu": 0.30610854780951674,
          "codebleu_details": {
            "codebleu": 0.30610854780951674,
            "ngram_match_score": 0.139780203108372,
            "weighted_ngram_match_score": 0.15140723488294158,
            "syntax_match_score": 0.68,
            "dataflow_match_score": 0.2532467532467532
          },
          "edit_distance": 1376,
          "normalized_edit_distance": 0.7122153209109731,
          "edit_similarity": 0.28778467908902694,
          "reference_length": 1932,
          "prediction_length": 1221
        }
      ]
    },
    {
      "project_name": "repoeval_can",
      "num_cases": 2,
      "average_codebleu": 0.2050389123401678,
      "average_edit_distance": 1049.0,
      "average_edit_similarity": 0.22472715149026185,
      "case_results": [
        {
          "codebleu": 0.23762680207105275,
          "codebleu_details": {
            "codebleu": 0.23762680207105275,
            "ngram_match_score": 0.1153211351880827,
            "weighted_ngram_match_score": 0.1543684630332352,
            "syntax_match_score": 0.5141509433962265,
            "dataflow_match_score": 0.16666666666666666
          },
          "edit_distance": 920,
          "normalized_edit_distance": 0.7221350078492935,
          "edit_similarity": 0.2778649921507065,
          "reference_length": 1274,
          "prediction_length": 492
        },
        {
          "codebleu": 0.17245102260928286,
          "codebleu_details": {
            "codebleu": 0.17245102260928286,
            "ngram_match_score": 0.09208553450428193,
            "weighted_ngram_match_score": 0.13550492432221536,
            "syntax_match_score": 0.2966507177033493,
            "dataflow_match_score": 0.16556291390728478
          },
          "edit_distance": 1178,
          "normalized_edit_distance": 0.8284106891701828,
          "edit_similarity": 0.1715893108298172,
          "reference_length": 1411,
          "prediction_length": 1422
        }
      ]
    },
    {
      "project_name": "repoeval_core",
      "num_cases": 2,
      "average_codebleu": 0.5506998771608612,
      "average_edit_distance": 638.0,
      "average_edit_similarity": 0.5654789583817716,
      "case_results": [
        {
          "codebleu": 0.5684814726499713,
          "codebleu_details": {
            "codebleu": 0.5684814726499713,
            "ngram_match_score": 0.36057171622331713,
            "weighted_ngram_match_score": 0.7801259042832236,
            "syntax_match_score": 0.831858407079646,
            "dataflow_match_score": 0.3013698630136986
          },
          "edit_distance": 826,
          "normalized_edit_distance": 0.5281329923273658,
          "edit_similarity": 0.47186700767263423,
          "reference_length": 778,
          "prediction_length": 1564
        },
        {
          "codebleu": 0.5329182816717511,
          "codebleu_details": {
            "codebleu": 0.5329182816717511,
            "ngram_match_score": 0.49000519944675514,
            "weighted_ngram_match_score": 0.5867525996419156,
            "syntax_match_score": 0.8469945355191257,
            "dataflow_match_score": 0.2079207920792079
          },
          "edit_distance": 450,
          "normalized_edit_distance": 0.3409090909090909,
          "edit_similarity": 0.6590909090909092,
          "reference_length": 1320,
          "prediction_length": 1215
        }
      ]
    },
    {
      "project_name": "repoeval_counter",
      "num_cases": 1,
      "average_codebleu": 0.6214917231739312,
      "average_edit_distance": 858.0,
      "average_edit_similarity": 0.23392857142857137,
      "case_results": [
        {
          "codebleu": 0.6214917231739312,
          "codebleu_details": {
            "codebleu": 0.6214917231739312,
            "ngram_match_score": 0.08883004364980324,
            "weighted_ngram_match_score": 0.5202137721228447,
            "syntax_match_score": 0.8769230769230769,
            "dataflow_match_score": 1.0
          },
          "edit_distance": 858,
          "normalized_edit_distance": 0.7660714285714286,
          "edit_similarity": 0.23392857142857137,
          "reference_length": 313,
          "prediction_length": 1120
        }
      ]
    },
    {
      "project_name": "repoeval_elevator",
      "num_cases": 5,
      "average_codebleu": 0.6292871199469449,
      "average_edit_distance": 550.0,
      "average_edit_similarity": 0.5236458132644358,
      "case_results": [
        {
          "codebleu": 0.5813502676952823,
          "codebleu_details": {
            "codebleu": 0.5813502676952823,
            "ngram_match_score": 0.38290985440834024,
            "weighted_ngram_match_score": 0.38445349746843327,
            "syntax_match_score": 0.7786259541984732,
            "dataflow_match_score": 0.7794117647058824
          },
          "edit_distance": 573,
          "normalized_edit_distance": 0.5075287865367581,
          "edit_similarity": 0.49247121346324185,
          "reference_length": 1128,
          "prediction_length": 1129
        },
        {
          "codebleu": 0.38119581827576415,
          "codebleu_details": {
            "codebleu": 0.38119581827576415,
            "ngram_match_score": 0.26549162202218063,
            "weighted_ngram_match_score": 0.29777934317933935,
            "syntax_match_score": 0.5813953488372093,
            "dataflow_match_score": 0.38011695906432746
          },
          "edit_distance": 1151,
          "normalized_edit_distance": 0.5920781893004116,
          "edit_similarity": 0.40792181069958844,
          "reference_length": 1944,
          "prediction_length": 1199
        },
        {
          "codebleu": 0.7066754414379814,
          "codebleu_details": {
            "codebleu": 0.7066754414379814,
            "ngram_match_score": 0.40014959378383796,
            "weighted_ngram_match_score": 0.694733990149906,
            "syntax_match_score": 0.9818181818181818,
            "dataflow_match_score": 0.75
          },
          "edit_distance": 372,
          "normalized_edit_distance": 0.4714828897338403,
          "edit_similarity": 0.5285171102661597,
          "reference_length": 517,
          "prediction_length": 789
        },
        {
          "codebleu": 0.8065704766844805,
          "codebleu_details": {
            "codebleu": 0.8065704766844805,
            "ngram_match_score": 0.5672704449465005,
            "weighted_ngram_match_score": 0.8071596099395696,
            "syntax_match_score": 0.8888888888888888,
            "dataflow_match_score": 0.9629629629629629
          },
          "edit_distance": 165,
          "normalized_edit_distance": 0.2727272727272727,
          "edit_similarity": 0.7272727272727273,
          "reference_length": 462,
          "prediction_length": 605
        },
        {
          "codebleu": 0.6706435956412158,
          "codebleu_details": {
            "codebleu": 0.6706435956412158,
            "ngram_match_score": 0.3241146693084428,
            "weighted_ngram_match_score": 0.557177661974369,
            "syntax_match_score": 0.9166666666666666,
            "dataflow_match_score": 0.8846153846153846
          },
          "edit_distance": 489,
          "normalized_edit_distance": 0.5379537953795379,
          "edit_similarity": 0.4620462046204621,
          "reference_length": 499,
          "prediction_length": 909
        }
      ]
    },
    {
      "project_name": "repoeval_healthydata",
      "num_cases": 4,
      "average_codebleu": 0.5667598603718917,
      "average_edit_distance": 1116.0,
      "average_edit_similarity": 0.3478975021339772,
      "case_results": [
        {
          "codebleu": 0.6078429437944731,
          "codebleu_details": {
            "codebleu": 0.6078429437944731,
            "ngram_match_score": 0.10049859344810229,
            "weighted_ngram_match_score": 0.6975398483964567,
            "syntax_match_score": 0.8,
            "dataflow_match_score": 0.8333333333333334
          },
          "edit_distance": 1477,
          "normalized_edit_distance": 0.854251012145749,
          "edit_similarity": 0.14574898785425106,
          "reference_length": 255,
          "prediction_length": 1729
        },
        {
          "codebleu": 0.571459072412237,
          "codebleu_details": {
            "codebleu": 0.571459072412237,
            "ngram_match_score": 0.09677428683829081,
            "weighted_ngram_match_score": 0.5981529119015665,
            "syntax_match_score": 0.9545454545454546,
            "dataflow_match_score": 0.6363636363636364
          },
          "edit_distance": 1462,
          "normalized_edit_distance": 0.8195067264573991,
          "edit_similarity": 0.18049327354260092,
          "reference_length": 342,
          "prediction_length": 1784
        },
        {
          "codebleu": 0.47780244330393196,
          "codebleu_details": {
            "codebleu": 0.47780244330393196,
            "ngram_match_score": 0.34036460005722147,
            "weighted_ngram_match_score": 0.44029229486182725,
            "syntax_match_score": 0.7345132743362832,
            "dataflow_match_score": 0.39603960396039606
          },
          "edit_distance": 939,
          "normalized_edit_distance": 0.52428810720268,
          "edit_similarity": 0.47571189279731996,
          "reference_length": 1791,
          "prediction_length": 1720
        },
        {
          "codebleu": 0.6099349819769249,
          "codebleu_details": {
            "codebleu": 0.6099349819769249,
            "ngram_match_score": 0.31819653969457834,
            "weighted_ngram_match_score": 0.6224508292294547,
            "syntax_match_score": 0.9473684210526315,
            "dataflow_match_score": 0.5517241379310345
          },
          "edit_distance": 586,
          "normalized_edit_distance": 0.4103641456582633,
          "edit_similarity": 0.5896358543417367,
          "reference_length": 937,
          "prediction_length": 1428
        }
      ]
    },
    {
      "project_name": "repoeval_isScaleOutput",
      "num_cases": 1,
      "average_codebleu": 0.8266417458720188,
      "average_edit_distance": 57.0,
      "average_edit_similarity": 0.9304878048780487,
      "case_results": [
        {
          "codebleu": 0.8266417458720188,
          "codebleu_details": {
            "codebleu": 0.8266417458720188,
            "ngram_match_score": 0.6252900271521832,
            "weighted_ngram_match_score": 0.7078256289022634,
            "syntax_match_score": 0.9734513274336283,
            "dataflow_match_score": 1.0
          },
          "edit_distance": 57,
          "normalized_edit_distance": 0.06951219512195123,
          "edit_similarity": 0.9304878048780487,
          "reference_length": 798,
          "prediction_length": 820
        }
      ]
    },
    {
      "project_name": "repoeval_modbus",
      "num_cases": 5,
      "average_codebleu": 0.39171665962132113,
      "average_edit_distance": 1506.8,
      "average_edit_similarity": 0.30839941237885304,
      "case_results": [
        {
          "codebleu": 0.672765147707685,
          "codebleu_details": {
            "codebleu": 0.672765147707685,
            "ngram_match_score": 0.22559933282277986,
            "weighted_ngram_match_score": 0.7511755437222457,
            "syntax_match_score": 0.9642857142857143,
            "dataflow_match_score": 0.75
          },
          "edit_distance": 380,
          "normalized_edit_distance": 0.6270627062706271,
          "edit_similarity": 0.3729372937293729,
          "reference_length": 236,
          "prediction_length": 606
        },
        {
          "codebleu": 0.35404327051446055,
          "codebleu_details": {
            "codebleu": 0.35404327051446055,
            "ngram_match_score": 0.1650751651563493,
            "weighted_ngram_match_score": 0.21210420620966905,
            "syntax_match_score": 0.4666666666666667,
            "dataflow_match_score": 0.5723270440251572
          },
          "edit_distance": 3071,
          "normalized_edit_distance": 0.7534347399411188,
          "edit_similarity": 0.24656526005888124,
          "reference_length": 2051,
          "prediction_length": 4076
        },
        {
          "codebleu": 0.17040545168272225,
          "codebleu_details": {
            "codebleu": 0.17040545168272225,
            "ngram_match_score": 0.04867308852258698,
            "weighted_ngram_match_score": 0.09798473261406433,
            "syntax_match_score": 0.33088235294117646,
            "dataflow_match_score": 0.20408163265306123
          },
          "edit_distance": 597,
          "normalized_edit_distance": 0.6869965477560415,
          "edit_similarity": 0.31300345224395854,
          "reference_length": 869,
          "prediction_length": 451
        },
        {
          "codebleu": 0.4150042168593929,
          "codebleu_details": {
            "codebleu": 0.4150042168593929,
            "ngram_match_score": 0.28847090391577523,
            "weighted_ngram_match_score": 0.30406407946382563,
            "syntax_match_score": 0.4945652173913043,
            "dataflow_match_score": 0.5729166666666666
          },
          "edit_distance": 1510,
          "normalized_edit_distance": 0.5882352941176471,
          "edit_similarity": 0.4117647058823529,
          "reference_length": 2078,
          "prediction_length": 2567
        },
        {
          "codebleu": 0.3463652113423449,
          "codebleu_details": {
            "codebleu": 0.3463652113423449,
            "ngram_match_score": 0.10607309336536024,
            "weighted_ngram_match_score": 0.215454299835273,
            "syntax_match_score": 0.6417112299465241,
            "dataflow_match_score": 0.4222222222222222
          },
          "edit_distance": 1976,
          "normalized_edit_distance": 0.8022736500203005,
          "edit_similarity": 0.1977263499796995,
          "reference_length": 1158,
          "prediction_length": 2463
        }
      ]
    },
    {
      "project_name": "repoeval_plc_hello_mixing_tank",
      "num_cases": 6,
      "average_codebleu": 0.6399547486802191,
      "average_edit_distance": 925.8333333333334,
      "average_edit_similarity": 0.4983050637622877,
      "case_results": [
        {
          "codebleu": 0.6278419904870259,
          "codebleu_details": {
            "codebleu": 0.6278419904870259,
            "ngram_match_score": 0.503517736823811,
            "weighted_ngram_match_score": 0.5347927811310457,
            "syntax_match_score": 0.8294930875576036,
            "dataflow_match_score": 0.6435643564356436
          },
          "edit_distance": 1021,
          "normalized_edit_distance": 0.47181146025878,
          "edit_similarity": 0.52818853974122,
          "reference_length": 2164,
          "prediction_length": 1923
        },
        {
          "codebleu": 0.5983262250981631,
          "codebleu_details": {
            "codebleu": 0.5983262250981631,
            "ngram_match_score": 0.3723582244133063,
            "weighted_ngram_match_score": 0.6265022315349016,
            "syntax_match_score": 0.7444444444444445,
            "dataflow_match_score": 0.65
          },
          "edit_distance": 774,
          "normalized_edit_distance": 0.4980694980694981,
          "edit_similarity": 0.5019305019305019,
          "reference_length": 930,
          "prediction_length": 1554
        },
        {
          "codebleu": 0.630222921248998,
          "codebleu_details": {
            "codebleu": 0.630222921248998,
            "ngram_match_score": 0.43272564937502356,
            "weighted_ngram_match_score": 0.5486055960605292,
            "syntax_match_score": 0.7538461538461538,
            "dataflow_match_score": 0.7857142857142857
          },
          "edit_distance": 248,
          "normalized_edit_distance": 0.4268502581755594,
          "edit_similarity": 0.5731497418244407,
          "reference_length": 523,
          "prediction_length": 581
        },
        {
          "codebleu": 0.7778045415987486,
          "codebleu_details": {
            "codebleu": 0.7778045415987486,
            "ngram_match_score": 0.5359194121362177,
            "weighted_ngram_match_score": 0.7613452658866836,
            "syntax_match_score": 0.813953488372093,
            "dataflow_match_score": 1.0
          },
          "edit_distance": 484,
          "normalized_edit_distance": 0.39190283400809717,
          "edit_similarity": 0.6080971659919028,
          "reference_length": 893,
          "prediction_length": 1235
        },
        {
          "codebleu": 0.6817717743701173,
          "codebleu_details": {
            "codebleu": 0.6817717743701173,
            "ngram_match_score": 0.5283507410939812,
            "weighted_ngram_match_score": 0.5892355512335088,
            "syntax_match_score": 0.8703703703703703,
            "dataflow_match_score": 0.7391304347826086
          },
          "edit_distance": 400,
          "normalized_edit_distance": 0.34873583260680036,
          "edit_similarity": 0.6512641673931996,
          "reference_length": 984,
          "prediction_length": 1147
        },
        {
          "codebleu": 0.5237610392782615,
          "codebleu_details": {
            "codebleu": 0.5237610392782615,
            "ngram_match_score": 0.13741214486063655,
            "weighted_ngram_match_score": 0.6135760681964653,
            "syntax_match_score": 0.7076923076923077,
            "dataflow_match_score": 0.6363636363636364
          },
          "edit_distance": 2628,
          "normalized_edit_distance": 0.872799734307539,
          "edit_similarity": 0.127200265692461,
          "reference_length": 444,
          "prediction_length": 3011
        }
      ]
    },
    {
      "project_name": "repoeval_三轴CNC运动实训",
      "num_cases": 5,
      "average_codebleu": 0.5739477610523247,
      "average_edit_distance": 808.2,
      "average_edit_similarity": 0.5890269523950986,
      "case_results": [
        {
          "codebleu": 0.4930709852531673,
          "codebleu_details": {
            "codebleu": 0.4930709852531673,
            "ngram_match_score": 0.3161015088451009,
            "weighted_ngram_match_score": 0.34266708920727945,
            "syntax_match_score": 0.8447653429602888,
            "dataflow_match_score": 0.46875
          },
          "edit_distance": 1117,
          "normalized_edit_distance": 0.4124815361890694,
          "edit_similarity": 0.5875184638109305,
          "reference_length": 2708,
          "prediction_length": 1820
        },
        {
          "codebleu": 0.5244819816553075,
          "codebleu_details": {
            "codebleu": 0.5244819816553075,
            "ngram_match_score": 0.4590618170846024,
            "weighted_ngram_match_score": 0.4800352563914326,
            "syntax_match_score": 0.8023952095808383,
            "dataflow_match_score": 0.3564356435643564
          },
          "edit_distance": 680,
          "normalized_edit_distance": 0.422360248447205,
          "edit_similarity": 0.577639751552795,
          "reference_length": 1610,
          "prediction_length": 1097
        },
        {
          "codebleu": 0.663356131094824,
          "codebleu_details": {
            "codebleu": 0.663356131094824,
            "ngram_match_score": 0.600813765894157,
            "weighted_ngram_match_score": 0.6037575982225829,
            "syntax_match_score": 0.7785234899328859,
            "dataflow_match_score": 0.6703296703296703
          },
          "edit_distance": 410,
          "normalized_edit_distance": 0.3195635229929852,
          "edit_similarity": 0.6804364770070148,
          "reference_length": 1283,
          "prediction_length": 1029
        },
        {
          "codebleu": 0.6314811333424732,
          "codebleu_details": {
            "codebleu": 0.6314811333424732,
            "ngram_match_score": 0.5252477796545503,
            "weighted_ngram_match_score": 0.5370854709279228,
            "syntax_match_score": 0.8037974683544303,
            "dataflow_match_score": 0.6597938144329897
          },
          "edit_distance": 488,
          "normalized_edit_distance": 0.3488205861329521,
          "edit_similarity": 0.6511794138670479,
          "reference_length": 1399,
          "prediction_length": 1103
        },
        {
          "codebleu": 0.5573485739158512,
          "codebleu_details": {
            "codebleu": 0.5573485739158512,
            "ngram_match_score": 0.24337321823339003,
            "weighted_ngram_match_score": 0.4273356314206253,
            "syntax_match_score": 0.7253521126760564,
            "dataflow_match_score": 0.8333333333333334
          },
          "edit_distance": 1346,
          "normalized_edit_distance": 0.5516393442622951,
          "edit_similarity": 0.4483606557377049,
          "reference_length": 1559,
          "prediction_length": 2440
        }
      ]
    },
    {
      "project_name": "repoeval_交通信号灯控制实训",
      "num_cases": 3,
      "average_codebleu": 0.6478192678353584,
      "average_edit_distance": 581.6666666666666,
      "average_edit_similarity": 0.5661872251490884,
      "case_results": [
        {
          "codebleu": 0.625070934201875,
          "codebleu_details": {
            "codebleu": 0.625070934201875,
            "ngram_match_score": 0.08956474621766805,
            "weighted_ngram_match_score": 0.4282628502389549,
            "syntax_match_score": 0.9824561403508771,
            "dataflow_match_score": 1.0
          },
          "edit_distance": 1648,
          "normalized_edit_distance": 0.8150346191889218,
          "edit_similarity": 0.18496538081107816,
          "reference_length": 492,
          "prediction_length": 2022
        },
        {
          "codebleu": 0.8072541342896562,
          "codebleu_details": {
            "codebleu": 0.8072541342896562,
            "ngram_match_score": 0.6712403123245675,
            "weighted_ngram_match_score": 0.7482524153102478,
            "syntax_match_score": 0.8095238095238095,
            "dataflow_match_score": 1.0
          },
          "edit_distance": 42,
          "normalized_edit_distance": 0.2028985507246377,
          "edit_similarity": 0.7971014492753623,
          "reference_length": 170,
          "prediction_length": 207
        },
        {
          "codebleu": 0.5111327350145437,
          "codebleu_details": {
            "codebleu": 0.5111327350145437,
            "ngram_match_score": 0.5613959048116961,
            "weighted_ngram_match_score": 0.5613959048116961,
            "syntax_match_score": 0.5217391304347826,
            "dataflow_match_score": 0.4
          },
          "edit_distance": 55,
          "normalized_edit_distance": 0.28350515463917525,
          "edit_similarity": 0.7164948453608248,
          "reference_length": 194,
          "prediction_length": 161
        }
      ]
    },
    {
      "project_name": "repoeval_四层电梯控制实训",
      "num_cases": 3,
      "average_codebleu": 0.5225460992800953,
      "average_edit_distance": 844.6666666666666,
      "average_edit_similarity": 0.3793202662409736,
      "case_results": [
        {
          "codebleu": 0.6716329850850007,
          "codebleu_details": {
            "codebleu": 0.6716329850850007,
            "ngram_match_score": 0.38071691336242286,
            "weighted_ngram_match_score": 0.5505702717328246,
            "syntax_match_score": 0.8461538461538461,
            "dataflow_match_score": 0.9090909090909091
          },
          "edit_distance": 318,
          "normalized_edit_distance": 0.4404432132963989,
          "edit_similarity": 0.5595567867036011,
          "reference_length": 601,
          "prediction_length": 722
        },
        {
          "codebleu": 0.433937827301914,
          "codebleu_details": {
            "codebleu": 0.433937827301914,
            "ngram_match_score": 0.121297409716182,
            "weighted_ngram_match_score": 0.5153548003923748,
            "syntax_match_score": 0.6666666666666666,
            "dataflow_match_score": 0.43243243243243246
          },
          "edit_distance": 1889,
          "normalized_edit_distance": 0.8011026293469041,
          "edit_similarity": 0.19889737065309587,
          "reference_length": 590,
          "prediction_length": 2358
        },
        {
          "codebleu": 0.46206748545337106,
          "codebleu_details": {
            "codebleu": 0.46206748545337106,
            "ngram_match_score": 0.2829375739367894,
            "weighted_ngram_match_score": 0.41255459009891693,
            "syntax_match_score": 0.7777777777777778,
            "dataflow_match_score": 0.375
          },
          "edit_distance": 327,
          "normalized_edit_distance": 0.6204933586337761,
          "edit_similarity": 0.3795066413662239,
          "reference_length": 400,
          "prediction_length": 527
        }
      ]
    },
    {
      "project_name": "repoeval_开放式双轴卷绕机编程开发实训",
      "num_cases": 5,
      "average_codebleu": 0.502465925501711,
      "average_edit_distance": 919.4,
      "average_edit_similarity": 0.4010068671708854,
      "case_results": [
        {
          "codebleu": 0.6999755857608938,
          "codebleu_details": {
            "codebleu": 0.6999755857608938,
            "ngram_match_score": 0.4355507009918242,
            "weighted_ngram_match_score": 0.7697570474571566,
            "syntax_match_score": 0.5945945945945946,
            "dataflow_match_score": 1.0
          },
          "edit_distance": 69,
          "normalized_edit_distance": 0.31221719457013575,
          "edit_similarity": 0.6877828054298643,
          "reference_length": 178,
          "prediction_length": 221
        },
        {
          "codebleu": 0.4717447289372019,
          "codebleu_details": {
            "codebleu": 0.4717447289372019,
            "ngram_match_score": 0.0898525711980623,
            "weighted_ngram_match_score": 0.6173017831472366,
            "syntax_match_score": 0.5131578947368421,
            "dataflow_match_score": 0.6666666666666666
          },
          "edit_distance": 1671,
          "normalized_edit_distance": 0.838855421686747,
          "edit_similarity": 0.16114457831325302,
          "reference_length": 398,
          "prediction_length": 1992
        },
        {
          "codebleu": 0.4188444471493965,
          "codebleu_details": {
            "codebleu": 0.4188444471493965,
            "ngram_match_score": 0.33430069440187293,
            "weighted_ngram_match_score": 0.3371755335714635,
            "syntax_match_score": 0.7244897959183674,
            "dataflow_match_score": 0.27941176470588236
          },
          "edit_distance": 405,
          "normalized_edit_distance": 0.5179028132992327,
          "edit_similarity": 0.4820971867007673,
          "reference_length": 782,
          "prediction_length": 734
        },
        {
          "codebleu": 0.5033854992966791,
          "codebleu_details": {
            "codebleu": 0.5033854992966791,
            "ngram_match_score": 0.36267047061518154,
            "weighted_ngram_match_score": 0.43653443714453866,
            "syntax_match_score": 0.718213058419244,
            "dataflow_match_score": 0.49612403100775193
          },
          "edit_distance": 1601,
          "normalized_edit_distance": 0.5781870711448176,
          "edit_similarity": 0.4218129288551824,
          "reference_length": 2609,
          "prediction_length": 2769
        },
        {
          "codebleu": 0.41837936636438355,
          "codebleu_details": {
            "codebleu": 0.41837936636438355,
            "ngram_match_score": 0.062152577085915074,
            "weighted_ngram_match_score": 0.23961164161837228,
            "syntax_match_score": 0.9431818181818182,
            "dataflow_match_score": 0.42857142857142855
          },
          "edit_distance": 851,
          "normalized_edit_distance": 0.7478031634446397,
          "edit_similarity": 0.2521968365553603,
          "reference_length": 577,
          "prediction_length": 1138
        }
      ]
    },
    {
      "project_name": "repoeval_测量控件",
      "num_cases": 1,
      "average_codebleu": 0.42221535419195616,
      "average_edit_distance": 902.0,
      "average_edit_similarity": 0.29586260733801717,
      "case_results": [
        {
          "codebleu": 0.42221535419195616,
          "codebleu_details": {
            "codebleu": 0.42221535419195616,
            "ngram_match_score": 0.13195496033917772,
            "weighted_ngram_match_score": 0.3124620119842024,
            "syntax_match_score": 0.6666666666666666,
            "dataflow_match_score": 0.5777777777777777
          },
          "edit_distance": 902,
          "normalized_edit_distance": 0.7041373926619828,
          "edit_similarity": 0.29586260733801717,
          "reference_length": 673,
          "prediction_length": 1281
        }
      ]
    },
    {
      "project_name": "repoeval_电子凸轮运动实训",
      "num_cases": 4,
      "average_codebleu": 0.4375370488746017,
      "average_edit_distance": 1144.5,
      "average_edit_similarity": 0.35088258477416256,
      "case_results": [
        {
          "codebleu": 0.6065772980656411,
          "codebleu_details": {
            "codebleu": 0.6065772980656411,
            "ngram_match_score": 0.24311075318782543,
            "weighted_ngram_match_score": 0.5269892887479414,
            "syntax_match_score": 0.7444444444444445,
            "dataflow_match_score": 0.9117647058823529
          },
          "edit_distance": 908,
          "normalized_edit_distance": 0.6106254203093476,
          "edit_similarity": 0.38937457969065237,
          "reference_length": 761,
          "prediction_length": 1487
        },
        {
          "codebleu": 0.40590790490647827,
          "codebleu_details": {
            "codebleu": 0.40590790490647827,
            "ngram_match_score": 0.18503097298082635,
            "weighted_ngram_match_score": 0.23031600429315868,
            "syntax_match_score": 0.8209606986899564,
            "dataflow_match_score": 0.3873239436619718
          },
          "edit_distance": 1242,
          "normalized_edit_distance": 0.6359447004608295,
          "edit_similarity": 0.3640552995391705,
          "reference_length": 1953,
          "prediction_length": 1002
        },
        {
          "codebleu": 0.5077475187707756,
          "codebleu_details": {
            "codebleu": 0.5077475187707756,
            "ngram_match_score": 0.15315610868829593,
            "weighted_ngram_match_score": 0.17195161345363008,
            "syntax_match_score": 0.9411764705882353,
            "dataflow_match_score": 0.7647058823529411
          },
          "edit_distance": 500,
          "normalized_edit_distance": 0.5387931034482759,
          "edit_similarity": 0.4612068965517241,
          "reference_length": 899,
          "prediction_length": 928
        },
        {
          "codebleu": 0.22991547375551183,
          "codebleu_details": {
            "codebleu": 0.22991547375551183,
            "ngram_match_score": 0.039335023310136816,
            "weighted_ngram_match_score": 0.07561708730063361,
            "syntax_match_score": 0.6355555555555555,
            "dataflow_match_score": 0.1691542288557214
          },
          "edit_distance": 1928,
          "normalized_edit_distance": 0.8111064366848969,
          "edit_similarity": 0.1888935633151031,
          "reference_length": 2377,
          "prediction_length": 609
        }
      ]
    },
    {
      "project_name": "repoeval_电子齿轮运动实训",
      "num_cases": 3,
      "average_codebleu": 0.5264717986057269,
      "average_edit_distance": 1679.6666666666667,
      "average_edit_similarity": 0.3543920807057197,
      "case_results": [
        {
          "codebleu": 0.5157848444764845,
          "codebleu_details": {
            "codebleu": 0.5157848444764845,
            "ngram_match_score": 0.14103697802237214,
            "weighted_ngram_match_score": 0.43955553498375965,
            "syntax_match_score": 0.7472527472527473,
            "dataflow_match_score": 0.7352941176470589
          },
          "edit_distance": 1483,
          "normalized_edit_distance": 0.7291052114060964,
          "edit_similarity": 0.2708947885939036,
          "reference_length": 746,
          "prediction_length": 2034
        },
        {
          "codebleu": 0.5418148334396296,
          "codebleu_details": {
            "codebleu": 0.5418148334396296,
            "ngram_match_score": 0.20555328938271772,
            "weighted_ngram_match_score": 0.39978193496565717,
            "syntax_match_score": 0.9385474860335196,
            "dataflow_match_score": 0.6233766233766234
          },
          "edit_distance": 1902,
          "normalized_edit_distance": 0.6462793068297655,
          "edit_similarity": 0.35372069317023447,
          "reference_length": 1578,
          "prediction_length": 2943
        },
        {
          "codebleu": 0.5218157179010667,
          "codebleu_details": {
            "codebleu": 0.5218157179010667,
            "ngram_match_score": 0.3696076876746642,
            "weighted_ngram_match_score": 0.3699684503733038,
            "syntax_match_score": 0.8205128205128205,
            "dataflow_match_score": 0.5271739130434783
          },
          "edit_distance": 1654,
          "normalized_edit_distance": 0.561439239646979,
          "edit_similarity": 0.43856076035302105,
          "reference_length": 2946,
          "prediction_length": 2714
        }
      ]
    },
    {
      "project_name": "repoeval_读写文件",
      "num_cases": 2,
      "average_codebleu": 0.5902727207292564,
      "average_edit_distance": 524.0,
      "average_edit_similarity": 0.6183215919511897,
      "case_results": [
        {
          "codebleu": 0.5282497554896927,
          "codebleu_details": {
            "codebleu": 0.5282497554896927,
            "ngram_match_score": 0.33764329296212015,
            "weighted_ngram_match_score": 0.3541084318531433,
            "syntax_match_score": 0.8465346534653465,
            "dataflow_match_score": 0.5747126436781609
          },
          "edit_distance": 615,
          "normalized_edit_distance": 0.4489051094890511,
          "edit_similarity": 0.551094890510949,
          "reference_length": 1370,
          "prediction_length": 1027
        },
        {
          "codebleu": 0.6522956859688203,
          "codebleu_details": {
            "codebleu": 0.6522956859688203,
            "ngram_match_score": 0.5189583202446632,
            "weighted_ngram_match_score": 0.5335741773251993,
            "syntax_match_score": 0.8423645320197044,
            "dataflow_match_score": 0.7142857142857143
          },
          "edit_distance": 433,
          "normalized_edit_distance": 0.31445170660856936,
          "edit_similarity": 0.6855482933914306,
          "reference_length": 1377,
          "prediction_length": 1167
        }
      ]
    }
  ]
}